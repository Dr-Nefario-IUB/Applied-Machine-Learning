{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fOmqxSwoVei5"
   },
   "source": [
    "CSCI P-556: Applied Machine Learning\n",
    "\n",
    "Fall 2019\n",
    "\n",
    "Assignment 4\n",
    "\n",
    "Due: 11:59PM, December 6, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZy3rcBEeUC8"
   },
   "source": [
    "Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nMLcX2kmeUL8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt  \n",
    "import seaborn as sns\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>423</td>\n",
       "      <td>504</td>\n",
       "      <td>493</td>\n",
       "      <td>521</td>\n",
       "      <td>466</td>\n",
       "      <td>494</td>\n",
       "      <td>479</td>\n",
       "      <td>482</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>478</td>\n",
       "      <td>479</td>\n",
       "      <td>567</td>\n",
       "      <td>547</td>\n",
       "      <td>498</td>\n",
       "      <td>484</td>\n",
       "      <td>474</td>\n",
       "      <td>567</td>\n",
       "      <td>538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483</td>\n",
       "      <td>499</td>\n",
       "      <td>520</td>\n",
       "      <td>467</td>\n",
       "      <td>495</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>477</td>\n",
       "      <td>488</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>481</td>\n",
       "      <td>484</td>\n",
       "      <td>451</td>\n",
       "      <td>445</td>\n",
       "      <td>443</td>\n",
       "      <td>481</td>\n",
       "      <td>485</td>\n",
       "      <td>492</td>\n",
       "      <td>477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>487</td>\n",
       "      <td>486</td>\n",
       "      <td>495</td>\n",
       "      <td>481</td>\n",
       "      <td>421</td>\n",
       "      <td>481</td>\n",
       "      <td>499</td>\n",
       "      <td>478</td>\n",
       "      <td>489</td>\n",
       "      <td>482</td>\n",
       "      <td>...</td>\n",
       "      <td>477</td>\n",
       "      <td>511</td>\n",
       "      <td>245</td>\n",
       "      <td>522</td>\n",
       "      <td>480</td>\n",
       "      <td>483</td>\n",
       "      <td>493</td>\n",
       "      <td>421</td>\n",
       "      <td>488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480</td>\n",
       "      <td>427</td>\n",
       "      <td>531</td>\n",
       "      <td>458</td>\n",
       "      <td>544</td>\n",
       "      <td>492</td>\n",
       "      <td>489</td>\n",
       "      <td>477</td>\n",
       "      <td>478</td>\n",
       "      <td>482</td>\n",
       "      <td>...</td>\n",
       "      <td>483</td>\n",
       "      <td>471</td>\n",
       "      <td>313</td>\n",
       "      <td>490</td>\n",
       "      <td>414</td>\n",
       "      <td>480</td>\n",
       "      <td>516</td>\n",
       "      <td>495</td>\n",
       "      <td>469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>491</td>\n",
       "      <td>472</td>\n",
       "      <td>430</td>\n",
       "      <td>463</td>\n",
       "      <td>431</td>\n",
       "      <td>480</td>\n",
       "      <td>459</td>\n",
       "      <td>477</td>\n",
       "      <td>481</td>\n",
       "      <td>479</td>\n",
       "      <td>...</td>\n",
       "      <td>479</td>\n",
       "      <td>493</td>\n",
       "      <td>435</td>\n",
       "      <td>444</td>\n",
       "      <td>455</td>\n",
       "      <td>482</td>\n",
       "      <td>468</td>\n",
       "      <td>497</td>\n",
       "      <td>435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0     485     423     504     493     521     466     494     479     482   \n",
       "1     483     499     520     467     495     484     485     477     488   \n",
       "2     487     486     495     481     421     481     499     478     489   \n",
       "3     480     427     531     458     544     492     489     477     478   \n",
       "4     491     472     430     463     431     480     459     477     481   \n",
       "\n",
       "   feat_9  ...  feat_491  feat_492  feat_493  feat_494  feat_495  feat_496  \\\n",
       "0     471  ...       478       479       567       547       498       484   \n",
       "1     491  ...       481       484       451       445       443       481   \n",
       "2     482  ...       477       511       245       522       480       483   \n",
       "3     482  ...       483       471       313       490       414       480   \n",
       "4     479  ...       479       493       435       444       455       482   \n",
       "\n",
       "   feat_497  feat_498  feat_499  labels  \n",
       "0       474       567       538       0  \n",
       "1       485       492       477       1  \n",
       "2       493       421       488       1  \n",
       "3       516       495       469       0  \n",
       "4       468       497       435       1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('a4-train.csv', index_col=0)\n",
    "test_data=pd.read_csv('a4-test.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>486</td>\n",
       "      <td>497</td>\n",
       "      <td>494</td>\n",
       "      <td>477</td>\n",
       "      <td>582</td>\n",
       "      <td>478</td>\n",
       "      <td>535</td>\n",
       "      <td>477</td>\n",
       "      <td>496</td>\n",
       "      <td>480</td>\n",
       "      <td>...</td>\n",
       "      <td>485</td>\n",
       "      <td>473</td>\n",
       "      <td>576</td>\n",
       "      <td>521</td>\n",
       "      <td>493</td>\n",
       "      <td>481</td>\n",
       "      <td>485</td>\n",
       "      <td>490</td>\n",
       "      <td>478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>496</td>\n",
       "      <td>524</td>\n",
       "      <td>490</td>\n",
       "      <td>485</td>\n",
       "      <td>438</td>\n",
       "      <td>488</td>\n",
       "      <td>503</td>\n",
       "      <td>476</td>\n",
       "      <td>474</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>474</td>\n",
       "      <td>519</td>\n",
       "      <td>441</td>\n",
       "      <td>453</td>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "      <td>503</td>\n",
       "      <td>543</td>\n",
       "      <td>547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>486</td>\n",
       "      <td>465</td>\n",
       "      <td>481</td>\n",
       "      <td>467</td>\n",
       "      <td>529</td>\n",
       "      <td>484</td>\n",
       "      <td>464</td>\n",
       "      <td>476</td>\n",
       "      <td>508</td>\n",
       "      <td>474</td>\n",
       "      <td>...</td>\n",
       "      <td>482</td>\n",
       "      <td>454</td>\n",
       "      <td>712</td>\n",
       "      <td>425</td>\n",
       "      <td>518</td>\n",
       "      <td>479</td>\n",
       "      <td>466</td>\n",
       "      <td>494</td>\n",
       "      <td>470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479</td>\n",
       "      <td>485</td>\n",
       "      <td>502</td>\n",
       "      <td>489</td>\n",
       "      <td>487</td>\n",
       "      <td>478</td>\n",
       "      <td>402</td>\n",
       "      <td>477</td>\n",
       "      <td>500</td>\n",
       "      <td>473</td>\n",
       "      <td>...</td>\n",
       "      <td>470</td>\n",
       "      <td>491</td>\n",
       "      <td>381</td>\n",
       "      <td>532</td>\n",
       "      <td>469</td>\n",
       "      <td>488</td>\n",
       "      <td>487</td>\n",
       "      <td>539</td>\n",
       "      <td>546</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>482</td>\n",
       "      <td>485</td>\n",
       "      <td>551</td>\n",
       "      <td>475</td>\n",
       "      <td>443</td>\n",
       "      <td>475</td>\n",
       "      <td>456</td>\n",
       "      <td>475</td>\n",
       "      <td>494</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>484</td>\n",
       "      <td>479</td>\n",
       "      <td>574</td>\n",
       "      <td>509</td>\n",
       "      <td>509</td>\n",
       "      <td>473</td>\n",
       "      <td>483</td>\n",
       "      <td>545</td>\n",
       "      <td>490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0     486     497     494     477     582     478     535     477     496   \n",
       "1     496     524     490     485     438     488     503     476     474   \n",
       "2     486     465     481     467     529     484     464     476     508   \n",
       "3     479     485     502     489     487     478     402     477     500   \n",
       "4     482     485     551     475     443     475     456     475     494   \n",
       "\n",
       "   feat_9  ...  feat_491  feat_492  feat_493  feat_494  feat_495  feat_496  \\\n",
       "0     480  ...       485       473       576       521       493       481   \n",
       "1     491  ...       474       519       441       453       488       488   \n",
       "2     474  ...       482       454       712       425       518       479   \n",
       "3     473  ...       470       491       381       532       469       488   \n",
       "4     471  ...       484       479       574       509       509       473   \n",
       "\n",
       "   feat_497  feat_498  feat_499  labels  \n",
       "0       485       490       478       0  \n",
       "1       503       543       547       0  \n",
       "2       466       494       470       1  \n",
       "3       487       539       546       1  \n",
       "4       483       545       490       1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feat_0      0\n",
       "feat_1      0\n",
       "feat_2      0\n",
       "feat_3      0\n",
       "feat_4      0\n",
       "feat_5      0\n",
       "feat_6      0\n",
       "feat_7      0\n",
       "feat_8      0\n",
       "feat_9      0\n",
       "feat_10     0\n",
       "feat_11     0\n",
       "feat_12     0\n",
       "feat_13     0\n",
       "feat_14     0\n",
       "feat_15     0\n",
       "feat_16     0\n",
       "feat_17     0\n",
       "feat_18     0\n",
       "feat_19     0\n",
       "feat_20     0\n",
       "feat_21     0\n",
       "feat_22     0\n",
       "feat_23     0\n",
       "feat_24     0\n",
       "feat_25     0\n",
       "feat_26     0\n",
       "feat_27     0\n",
       "feat_28     0\n",
       "feat_29     0\n",
       "           ..\n",
       "feat_471    0\n",
       "feat_472    0\n",
       "feat_473    0\n",
       "feat_474    0\n",
       "feat_475    0\n",
       "feat_476    0\n",
       "feat_477    0\n",
       "feat_478    0\n",
       "feat_479    0\n",
       "feat_480    0\n",
       "feat_481    0\n",
       "feat_482    0\n",
       "feat_483    0\n",
       "feat_484    0\n",
       "feat_485    0\n",
       "feat_486    0\n",
       "feat_487    0\n",
       "feat_488    0\n",
       "feat_489    0\n",
       "feat_490    0\n",
       "feat_491    0\n",
       "feat_492    0\n",
       "feat_493    0\n",
       "feat_494    0\n",
       "feat_495    0\n",
       "feat_496    0\n",
       "feat_497    0\n",
       "feat_498    0\n",
       "feat_499    0\n",
       "labels      0\n",
       "Length: 501, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feat_0      0\n",
       "feat_1      0\n",
       "feat_2      0\n",
       "feat_3      0\n",
       "feat_4      0\n",
       "feat_5      0\n",
       "feat_6      0\n",
       "feat_7      0\n",
       "feat_8      0\n",
       "feat_9      0\n",
       "feat_10     0\n",
       "feat_11     0\n",
       "feat_12     0\n",
       "feat_13     0\n",
       "feat_14     0\n",
       "feat_15     0\n",
       "feat_16     0\n",
       "feat_17     0\n",
       "feat_18     0\n",
       "feat_19     0\n",
       "feat_20     0\n",
       "feat_21     0\n",
       "feat_22     0\n",
       "feat_23     0\n",
       "feat_24     0\n",
       "feat_25     0\n",
       "feat_26     0\n",
       "feat_27     0\n",
       "feat_28     0\n",
       "feat_29     0\n",
       "           ..\n",
       "feat_471    0\n",
       "feat_472    0\n",
       "feat_473    0\n",
       "feat_474    0\n",
       "feat_475    0\n",
       "feat_476    0\n",
       "feat_477    0\n",
       "feat_478    0\n",
       "feat_479    0\n",
       "feat_480    0\n",
       "feat_481    0\n",
       "feat_482    0\n",
       "feat_483    0\n",
       "feat_484    0\n",
       "feat_485    0\n",
       "feat_486    0\n",
       "feat_487    0\n",
       "feat_488    0\n",
       "feat_489    0\n",
       "feat_490    0\n",
       "feat_491    0\n",
       "feat_492    0\n",
       "feat_493    0\n",
       "feat_494    0\n",
       "feat_495    0\n",
       "feat_496    0\n",
       "feat_497    0\n",
       "feat_498    0\n",
       "feat_499    0\n",
       "labels      0\n",
       "Length: 501, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5',\n",
       "       'feat_6', 'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11',\n",
       "       'feat_12', 'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17',\n",
       "       'feat_18', 'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23',\n",
       "       'feat_24', 'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29',\n",
       "       'feat_30', 'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35',\n",
       "       'feat_36', 'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41',\n",
       "       'feat_42', 'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47',\n",
       "       'feat_48', 'feat_49', 'feat_50', 'feat_51', 'feat_52', 'feat_53',\n",
       "       'feat_54', 'feat_55', 'feat_56', 'feat_57', 'feat_58', 'feat_59',\n",
       "       'feat_60', 'feat_61', 'feat_62', 'feat_63', 'feat_64', 'feat_65',\n",
       "       'feat_66', 'feat_67', 'feat_68', 'feat_69', 'feat_70', 'feat_71',\n",
       "       'feat_72', 'feat_73', 'feat_74', 'feat_75', 'feat_76', 'feat_77',\n",
       "       'feat_78', 'feat_79', 'feat_80', 'feat_81', 'feat_82', 'feat_83',\n",
       "       'feat_84', 'feat_85', 'feat_86', 'feat_87', 'feat_88', 'feat_89',\n",
       "       'feat_90', 'feat_91', 'feat_92', 'feat_93', 'feat_94', 'feat_95',\n",
       "       'feat_96', 'feat_97', 'feat_98', 'feat_99', 'feat_100', 'feat_101',\n",
       "       'feat_102', 'feat_103', 'feat_104', 'feat_105', 'feat_106',\n",
       "       'feat_107', 'feat_108', 'feat_109', 'feat_110', 'feat_111',\n",
       "       'feat_112', 'feat_113', 'feat_114', 'feat_115', 'feat_116',\n",
       "       'feat_117', 'feat_118', 'feat_119', 'feat_120', 'feat_121',\n",
       "       'feat_122', 'feat_123', 'feat_124', 'feat_125', 'feat_126',\n",
       "       'feat_127', 'feat_128', 'feat_129', 'feat_130', 'feat_131',\n",
       "       'feat_132', 'feat_133', 'feat_134', 'feat_135', 'feat_136',\n",
       "       'feat_137', 'feat_138', 'feat_139', 'feat_140', 'feat_141',\n",
       "       'feat_142', 'feat_143', 'feat_144', 'feat_145', 'feat_146',\n",
       "       'feat_147', 'feat_148', 'feat_149', 'feat_150', 'feat_151',\n",
       "       'feat_152', 'feat_153', 'feat_154', 'feat_155', 'feat_156',\n",
       "       'feat_157', 'feat_158', 'feat_159', 'feat_160', 'feat_161',\n",
       "       'feat_162', 'feat_163', 'feat_164', 'feat_165', 'feat_166',\n",
       "       'feat_167', 'feat_168', 'feat_169', 'feat_170', 'feat_171',\n",
       "       'feat_172', 'feat_173', 'feat_174', 'feat_175', 'feat_176',\n",
       "       'feat_177', 'feat_178', 'feat_179', 'feat_180', 'feat_181',\n",
       "       'feat_182', 'feat_183', 'feat_184', 'feat_185', 'feat_186',\n",
       "       'feat_187', 'feat_188', 'feat_189', 'feat_190', 'feat_191',\n",
       "       'feat_192', 'feat_193', 'feat_194', 'feat_195', 'feat_196',\n",
       "       'feat_197', 'feat_198', 'feat_199', 'feat_200', 'feat_201',\n",
       "       'feat_202', 'feat_203', 'feat_204', 'feat_205', 'feat_206',\n",
       "       'feat_207', 'feat_208', 'feat_209', 'feat_210', 'feat_211',\n",
       "       'feat_212', 'feat_213', 'feat_214', 'feat_215', 'feat_216',\n",
       "       'feat_217', 'feat_218', 'feat_219', 'feat_220', 'feat_221',\n",
       "       'feat_222', 'feat_223', 'feat_224', 'feat_225', 'feat_226',\n",
       "       'feat_227', 'feat_228', 'feat_229', 'feat_230', 'feat_231',\n",
       "       'feat_232', 'feat_233', 'feat_234', 'feat_235', 'feat_236',\n",
       "       'feat_237', 'feat_238', 'feat_239', 'feat_240', 'feat_241',\n",
       "       'feat_242', 'feat_243', 'feat_244', 'feat_245', 'feat_246',\n",
       "       'feat_247', 'feat_248', 'feat_249', 'feat_250', 'feat_251',\n",
       "       'feat_252', 'feat_253', 'feat_254', 'feat_255', 'feat_256',\n",
       "       'feat_257', 'feat_258', 'feat_259', 'feat_260', 'feat_261',\n",
       "       'feat_262', 'feat_263', 'feat_264', 'feat_265', 'feat_266',\n",
       "       'feat_267', 'feat_268', 'feat_269', 'feat_270', 'feat_271',\n",
       "       'feat_272', 'feat_273', 'feat_274', 'feat_275', 'feat_276',\n",
       "       'feat_277', 'feat_278', 'feat_279', 'feat_280', 'feat_281',\n",
       "       'feat_282', 'feat_283', 'feat_284', 'feat_285', 'feat_286',\n",
       "       'feat_287', 'feat_288', 'feat_289', 'feat_290', 'feat_291',\n",
       "       'feat_292', 'feat_293', 'feat_294', 'feat_295', 'feat_296',\n",
       "       'feat_297', 'feat_298', 'feat_299', 'feat_300', 'feat_301',\n",
       "       'feat_302', 'feat_303', 'feat_304', 'feat_305', 'feat_306',\n",
       "       'feat_307', 'feat_308', 'feat_309', 'feat_310', 'feat_311',\n",
       "       'feat_312', 'feat_313', 'feat_314', 'feat_315', 'feat_316',\n",
       "       'feat_317', 'feat_318', 'feat_319', 'feat_320', 'feat_321',\n",
       "       'feat_322', 'feat_323', 'feat_324', 'feat_325', 'feat_326',\n",
       "       'feat_327', 'feat_328', 'feat_329', 'feat_330', 'feat_331',\n",
       "       'feat_332', 'feat_333', 'feat_334', 'feat_335', 'feat_336',\n",
       "       'feat_337', 'feat_338', 'feat_339', 'feat_340', 'feat_341',\n",
       "       'feat_342', 'feat_343', 'feat_344', 'feat_345', 'feat_346',\n",
       "       'feat_347', 'feat_348', 'feat_349', 'feat_350', 'feat_351',\n",
       "       'feat_352', 'feat_353', 'feat_354', 'feat_355', 'feat_356',\n",
       "       'feat_357', 'feat_358', 'feat_359', 'feat_360', 'feat_361',\n",
       "       'feat_362', 'feat_363', 'feat_364', 'feat_365', 'feat_366',\n",
       "       'feat_367', 'feat_368', 'feat_369', 'feat_370', 'feat_371',\n",
       "       'feat_372', 'feat_373', 'feat_374', 'feat_375', 'feat_376',\n",
       "       'feat_377', 'feat_378', 'feat_379', 'feat_380', 'feat_381',\n",
       "       'feat_382', 'feat_383', 'feat_384', 'feat_385', 'feat_386',\n",
       "       'feat_387', 'feat_388', 'feat_389', 'feat_390', 'feat_391',\n",
       "       'feat_392', 'feat_393', 'feat_394', 'feat_395', 'feat_396',\n",
       "       'feat_397', 'feat_398', 'feat_399', 'feat_400', 'feat_401',\n",
       "       'feat_402', 'feat_403', 'feat_404', 'feat_405', 'feat_406',\n",
       "       'feat_407', 'feat_408', 'feat_409', 'feat_410', 'feat_411',\n",
       "       'feat_412', 'feat_413', 'feat_414', 'feat_415', 'feat_416',\n",
       "       'feat_417', 'feat_418', 'feat_419', 'feat_420', 'feat_421',\n",
       "       'feat_422', 'feat_423', 'feat_424', 'feat_425', 'feat_426',\n",
       "       'feat_427', 'feat_428', 'feat_429', 'feat_430', 'feat_431',\n",
       "       'feat_432', 'feat_433', 'feat_434', 'feat_435', 'feat_436',\n",
       "       'feat_437', 'feat_438', 'feat_439', 'feat_440', 'feat_441',\n",
       "       'feat_442', 'feat_443', 'feat_444', 'feat_445', 'feat_446',\n",
       "       'feat_447', 'feat_448', 'feat_449', 'feat_450', 'feat_451',\n",
       "       'feat_452', 'feat_453', 'feat_454', 'feat_455', 'feat_456',\n",
       "       'feat_457', 'feat_458', 'feat_459', 'feat_460', 'feat_461',\n",
       "       'feat_462', 'feat_463', 'feat_464', 'feat_465', 'feat_466',\n",
       "       'feat_467', 'feat_468', 'feat_469', 'feat_470', 'feat_471',\n",
       "       'feat_472', 'feat_473', 'feat_474', 'feat_475', 'feat_476',\n",
       "       'feat_477', 'feat_478', 'feat_479', 'feat_480', 'feat_481',\n",
       "       'feat_482', 'feat_483', 'feat_484', 'feat_485', 'feat_486',\n",
       "       'feat_487', 'feat_488', 'feat_489', 'feat_490', 'feat_491',\n",
       "       'feat_492', 'feat_493', 'feat_494', 'feat_495', 'feat_496',\n",
       "       'feat_497', 'feat_498', 'feat_499', 'labels'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=data.columns.values\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 501)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zT3Oh9KcdWYd"
   },
   "source": [
    "Task 1: Perform exploratory data analysis (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "us3rhe68VPb_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x254096158d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis=list(data.labels.value_counts().index)\n",
    "y_axis=list(data.labels.value_counts().values)\n",
    "sns.barplot(x_axis,y_axis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we can deduce that there's an even distribution of samples of both classes in our data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feat_0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.042789</td>\n",
       "      <td>-0.022897</td>\n",
       "      <td>0.017871</td>\n",
       "      <td>-0.003337</td>\n",
       "      <td>0.017776</td>\n",
       "      <td>0.036649</td>\n",
       "      <td>0.046846</td>\n",
       "      <td>-0.024170</td>\n",
       "      <td>-0.012535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017542</td>\n",
       "      <td>-0.003865</td>\n",
       "      <td>0.015519</td>\n",
       "      <td>0.007361</td>\n",
       "      <td>-0.034734</td>\n",
       "      <td>0.013165</td>\n",
       "      <td>-0.033386</td>\n",
       "      <td>0.008648</td>\n",
       "      <td>-0.046535</td>\n",
       "      <td>0.002414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_1</th>\n",
       "      <td>0.042789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.008173</td>\n",
       "      <td>0.020494</td>\n",
       "      <td>0.029011</td>\n",
       "      <td>-0.014954</td>\n",
       "      <td>-0.032232</td>\n",
       "      <td>0.020178</td>\n",
       "      <td>0.015920</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008182</td>\n",
       "      <td>0.018399</td>\n",
       "      <td>0.026583</td>\n",
       "      <td>-0.002381</td>\n",
       "      <td>0.018728</td>\n",
       "      <td>0.043230</td>\n",
       "      <td>-0.006259</td>\n",
       "      <td>-0.001824</td>\n",
       "      <td>0.041259</td>\n",
       "      <td>0.019003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_2</th>\n",
       "      <td>-0.022897</td>\n",
       "      <td>-0.008173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009640</td>\n",
       "      <td>0.007198</td>\n",
       "      <td>0.016953</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>-0.018082</td>\n",
       "      <td>-0.004728</td>\n",
       "      <td>-0.018773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>-0.000458</td>\n",
       "      <td>-0.013680</td>\n",
       "      <td>0.009596</td>\n",
       "      <td>0.026388</td>\n",
       "      <td>0.010825</td>\n",
       "      <td>0.037516</td>\n",
       "      <td>0.022287</td>\n",
       "      <td>0.020370</td>\n",
       "      <td>0.011906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_3</th>\n",
       "      <td>0.017871</td>\n",
       "      <td>0.020494</td>\n",
       "      <td>-0.009640</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010786</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>-0.009176</td>\n",
       "      <td>-0.017608</td>\n",
       "      <td>-0.035731</td>\n",
       "      <td>0.010161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045861</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>0.017454</td>\n",
       "      <td>-0.007225</td>\n",
       "      <td>0.013877</td>\n",
       "      <td>-0.010965</td>\n",
       "      <td>-0.003627</td>\n",
       "      <td>-0.014491</td>\n",
       "      <td>-0.000589</td>\n",
       "      <td>-0.002153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_4</th>\n",
       "      <td>-0.003337</td>\n",
       "      <td>0.029011</td>\n",
       "      <td>0.007198</td>\n",
       "      <td>-0.010786</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.009717</td>\n",
       "      <td>0.024661</td>\n",
       "      <td>-0.058043</td>\n",
       "      <td>-0.027877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003109</td>\n",
       "      <td>-0.039532</td>\n",
       "      <td>-0.013767</td>\n",
       "      <td>0.031348</td>\n",
       "      <td>0.028628</td>\n",
       "      <td>0.053824</td>\n",
       "      <td>0.012479</td>\n",
       "      <td>0.026136</td>\n",
       "      <td>0.010764</td>\n",
       "      <td>0.042497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_5</th>\n",
       "      <td>0.017776</td>\n",
       "      <td>-0.014954</td>\n",
       "      <td>0.016953</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.047054</td>\n",
       "      <td>-0.024812</td>\n",
       "      <td>0.042148</td>\n",
       "      <td>0.022175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025443</td>\n",
       "      <td>-0.024741</td>\n",
       "      <td>-0.016164</td>\n",
       "      <td>-0.032220</td>\n",
       "      <td>0.028203</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.010998</td>\n",
       "      <td>-0.016103</td>\n",
       "      <td>0.017191</td>\n",
       "      <td>-0.031056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_6</th>\n",
       "      <td>0.036649</td>\n",
       "      <td>-0.032232</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>-0.009176</td>\n",
       "      <td>0.009717</td>\n",
       "      <td>-0.047054</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026023</td>\n",
       "      <td>-0.008711</td>\n",
       "      <td>-0.024414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.013658</td>\n",
       "      <td>-0.015206</td>\n",
       "      <td>0.019108</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>0.031925</td>\n",
       "      <td>-0.027418</td>\n",
       "      <td>0.026832</td>\n",
       "      <td>0.016975</td>\n",
       "      <td>-0.016405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_7</th>\n",
       "      <td>0.046846</td>\n",
       "      <td>0.020178</td>\n",
       "      <td>-0.018082</td>\n",
       "      <td>-0.017608</td>\n",
       "      <td>0.024661</td>\n",
       "      <td>-0.024812</td>\n",
       "      <td>0.026023</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.035779</td>\n",
       "      <td>0.016051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024678</td>\n",
       "      <td>0.021953</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.009076</td>\n",
       "      <td>0.016342</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.008581</td>\n",
       "      <td>0.014249</td>\n",
       "      <td>0.015172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_8</th>\n",
       "      <td>-0.024170</td>\n",
       "      <td>0.015920</td>\n",
       "      <td>-0.004728</td>\n",
       "      <td>-0.035731</td>\n",
       "      <td>-0.058043</td>\n",
       "      <td>0.042148</td>\n",
       "      <td>-0.008711</td>\n",
       "      <td>-0.035779</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002311</td>\n",
       "      <td>0.008186</td>\n",
       "      <td>0.027587</td>\n",
       "      <td>-0.013677</td>\n",
       "      <td>-0.008730</td>\n",
       "      <td>-0.032714</td>\n",
       "      <td>-0.009209</td>\n",
       "      <td>0.020083</td>\n",
       "      <td>0.034521</td>\n",
       "      <td>0.009541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_9</th>\n",
       "      <td>-0.012535</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>-0.018773</td>\n",
       "      <td>0.010161</td>\n",
       "      <td>-0.027877</td>\n",
       "      <td>0.022175</td>\n",
       "      <td>-0.024414</td>\n",
       "      <td>0.016051</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.042103</td>\n",
       "      <td>-0.033364</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>-0.022317</td>\n",
       "      <td>0.073471</td>\n",
       "      <td>-0.001086</td>\n",
       "      <td>0.010612</td>\n",
       "      <td>-0.047127</td>\n",
       "      <td>-0.009459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_10</th>\n",
       "      <td>-0.028289</td>\n",
       "      <td>-0.008287</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>-0.007691</td>\n",
       "      <td>-0.035444</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>-0.016498</td>\n",
       "      <td>0.024683</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>-0.023693</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005208</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>-0.022679</td>\n",
       "      <td>0.021158</td>\n",
       "      <td>-0.005951</td>\n",
       "      <td>-0.014404</td>\n",
       "      <td>0.028908</td>\n",
       "      <td>-0.026111</td>\n",
       "      <td>-0.048361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_11</th>\n",
       "      <td>0.046545</td>\n",
       "      <td>-0.013747</td>\n",
       "      <td>0.019508</td>\n",
       "      <td>-0.007620</td>\n",
       "      <td>0.044365</td>\n",
       "      <td>0.022871</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>-0.032809</td>\n",
       "      <td>-0.034526</td>\n",
       "      <td>0.025780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030249</td>\n",
       "      <td>-0.040590</td>\n",
       "      <td>0.012397</td>\n",
       "      <td>0.020546</td>\n",
       "      <td>0.039812</td>\n",
       "      <td>0.010577</td>\n",
       "      <td>0.028644</td>\n",
       "      <td>0.010386</td>\n",
       "      <td>-0.040684</td>\n",
       "      <td>0.005303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_12</th>\n",
       "      <td>0.035533</td>\n",
       "      <td>-0.016667</td>\n",
       "      <td>-0.025514</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.013538</td>\n",
       "      <td>-0.013960</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.018793</td>\n",
       "      <td>-0.026403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006102</td>\n",
       "      <td>-0.000917</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>0.046594</td>\n",
       "      <td>0.021604</td>\n",
       "      <td>0.017387</td>\n",
       "      <td>-0.021652</td>\n",
       "      <td>0.036468</td>\n",
       "      <td>-0.048082</td>\n",
       "      <td>0.031893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_13</th>\n",
       "      <td>0.035364</td>\n",
       "      <td>-0.002263</td>\n",
       "      <td>-0.004344</td>\n",
       "      <td>-0.013538</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.011463</td>\n",
       "      <td>0.030127</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.022669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009394</td>\n",
       "      <td>0.026111</td>\n",
       "      <td>-0.006425</td>\n",
       "      <td>-0.010519</td>\n",
       "      <td>0.036022</td>\n",
       "      <td>-0.002835</td>\n",
       "      <td>-0.007523</td>\n",
       "      <td>0.058581</td>\n",
       "      <td>-0.032736</td>\n",
       "      <td>-0.011137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_14</th>\n",
       "      <td>-0.002309</td>\n",
       "      <td>-0.015934</td>\n",
       "      <td>0.016829</td>\n",
       "      <td>-0.006832</td>\n",
       "      <td>-0.048193</td>\n",
       "      <td>0.020685</td>\n",
       "      <td>0.016099</td>\n",
       "      <td>-0.010436</td>\n",
       "      <td>-0.035499</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022314</td>\n",
       "      <td>0.022235</td>\n",
       "      <td>-0.007420</td>\n",
       "      <td>0.021198</td>\n",
       "      <td>-0.009774</td>\n",
       "      <td>-0.003077</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.030357</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>-0.023038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_15</th>\n",
       "      <td>0.022011</td>\n",
       "      <td>-0.006341</td>\n",
       "      <td>0.022330</td>\n",
       "      <td>-0.031577</td>\n",
       "      <td>-0.004214</td>\n",
       "      <td>-0.003504</td>\n",
       "      <td>0.021716</td>\n",
       "      <td>0.021439</td>\n",
       "      <td>-0.016426</td>\n",
       "      <td>-0.003054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007723</td>\n",
       "      <td>-0.007459</td>\n",
       "      <td>0.008166</td>\n",
       "      <td>-0.016099</td>\n",
       "      <td>0.062253</td>\n",
       "      <td>0.010355</td>\n",
       "      <td>-0.006185</td>\n",
       "      <td>-0.040172</td>\n",
       "      <td>-0.045842</td>\n",
       "      <td>0.015765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_16</th>\n",
       "      <td>-0.017550</td>\n",
       "      <td>-0.001521</td>\n",
       "      <td>-0.006896</td>\n",
       "      <td>0.048102</td>\n",
       "      <td>0.006396</td>\n",
       "      <td>0.033854</td>\n",
       "      <td>-0.030382</td>\n",
       "      <td>-0.006547</td>\n",
       "      <td>-0.010707</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>-0.028106</td>\n",
       "      <td>-0.011146</td>\n",
       "      <td>0.037487</td>\n",
       "      <td>-0.027471</td>\n",
       "      <td>0.036239</td>\n",
       "      <td>-0.018152</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.006333</td>\n",
       "      <td>-0.016765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_17</th>\n",
       "      <td>-0.023106</td>\n",
       "      <td>0.006835</td>\n",
       "      <td>-0.018727</td>\n",
       "      <td>-0.034937</td>\n",
       "      <td>0.008656</td>\n",
       "      <td>-0.023041</td>\n",
       "      <td>0.009720</td>\n",
       "      <td>-0.013263</td>\n",
       "      <td>0.006774</td>\n",
       "      <td>-0.002309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021513</td>\n",
       "      <td>-0.017691</td>\n",
       "      <td>-0.008233</td>\n",
       "      <td>-0.009109</td>\n",
       "      <td>0.015044</td>\n",
       "      <td>0.032901</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>-0.023839</td>\n",
       "      <td>-0.006149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_18</th>\n",
       "      <td>-0.035264</td>\n",
       "      <td>-0.007024</td>\n",
       "      <td>0.010042</td>\n",
       "      <td>0.013371</td>\n",
       "      <td>-0.002458</td>\n",
       "      <td>0.024916</td>\n",
       "      <td>0.062293</td>\n",
       "      <td>-0.026648</td>\n",
       "      <td>-0.011219</td>\n",
       "      <td>0.051074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031787</td>\n",
       "      <td>-0.008926</td>\n",
       "      <td>-0.035670</td>\n",
       "      <td>-0.015116</td>\n",
       "      <td>0.016753</td>\n",
       "      <td>0.002814</td>\n",
       "      <td>-0.009429</td>\n",
       "      <td>-0.031442</td>\n",
       "      <td>0.012699</td>\n",
       "      <td>0.031288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_19</th>\n",
       "      <td>0.033320</td>\n",
       "      <td>-0.004546</td>\n",
       "      <td>-0.001934</td>\n",
       "      <td>-0.035104</td>\n",
       "      <td>-0.008122</td>\n",
       "      <td>-0.019208</td>\n",
       "      <td>-0.011885</td>\n",
       "      <td>-0.047448</td>\n",
       "      <td>-0.019565</td>\n",
       "      <td>-0.016438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021215</td>\n",
       "      <td>-0.010067</td>\n",
       "      <td>0.013748</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>-0.013806</td>\n",
       "      <td>0.017365</td>\n",
       "      <td>0.025462</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>-0.027223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_20</th>\n",
       "      <td>0.011568</td>\n",
       "      <td>-0.050863</td>\n",
       "      <td>0.014647</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.008524</td>\n",
       "      <td>-0.002222</td>\n",
       "      <td>-0.035338</td>\n",
       "      <td>-0.041613</td>\n",
       "      <td>-0.008407</td>\n",
       "      <td>0.006416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004531</td>\n",
       "      <td>-0.013069</td>\n",
       "      <td>0.025213</td>\n",
       "      <td>0.015819</td>\n",
       "      <td>0.015118</td>\n",
       "      <td>0.018748</td>\n",
       "      <td>-0.031236</td>\n",
       "      <td>-0.002514</td>\n",
       "      <td>-0.014854</td>\n",
       "      <td>-0.014661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_21</th>\n",
       "      <td>-0.011032</td>\n",
       "      <td>0.002452</td>\n",
       "      <td>0.029135</td>\n",
       "      <td>-0.018933</td>\n",
       "      <td>0.034279</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.009685</td>\n",
       "      <td>-0.008820</td>\n",
       "      <td>0.009271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009258</td>\n",
       "      <td>0.012236</td>\n",
       "      <td>-0.000703</td>\n",
       "      <td>-0.028971</td>\n",
       "      <td>0.031428</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>-0.030445</td>\n",
       "      <td>0.017736</td>\n",
       "      <td>-0.014065</td>\n",
       "      <td>0.011415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_22</th>\n",
       "      <td>-0.011923</td>\n",
       "      <td>-0.017111</td>\n",
       "      <td>-0.003161</td>\n",
       "      <td>0.031944</td>\n",
       "      <td>-0.009345</td>\n",
       "      <td>-0.031033</td>\n",
       "      <td>-0.004462</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>-0.006769</td>\n",
       "      <td>-0.023220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012487</td>\n",
       "      <td>-0.052445</td>\n",
       "      <td>0.007643</td>\n",
       "      <td>-0.018234</td>\n",
       "      <td>-0.013856</td>\n",
       "      <td>0.016894</td>\n",
       "      <td>-0.030705</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>-0.016642</td>\n",
       "      <td>0.009651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_23</th>\n",
       "      <td>0.013634</td>\n",
       "      <td>0.037519</td>\n",
       "      <td>0.041670</td>\n",
       "      <td>-0.043839</td>\n",
       "      <td>0.007481</td>\n",
       "      <td>-0.043683</td>\n",
       "      <td>0.006662</td>\n",
       "      <td>0.033601</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>0.034675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011107</td>\n",
       "      <td>0.020871</td>\n",
       "      <td>0.016665</td>\n",
       "      <td>0.011878</td>\n",
       "      <td>0.018134</td>\n",
       "      <td>-0.016175</td>\n",
       "      <td>-0.003524</td>\n",
       "      <td>-0.020215</td>\n",
       "      <td>-0.029700</td>\n",
       "      <td>-0.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_24</th>\n",
       "      <td>-0.020635</td>\n",
       "      <td>0.003243</td>\n",
       "      <td>-0.019119</td>\n",
       "      <td>0.039047</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>-0.028574</td>\n",
       "      <td>0.029267</td>\n",
       "      <td>0.006652</td>\n",
       "      <td>0.015089</td>\n",
       "      <td>-0.052089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006032</td>\n",
       "      <td>0.010320</td>\n",
       "      <td>0.018516</td>\n",
       "      <td>0.029944</td>\n",
       "      <td>0.018211</td>\n",
       "      <td>-0.004224</td>\n",
       "      <td>-0.011241</td>\n",
       "      <td>-0.020421</td>\n",
       "      <td>-0.018546</td>\n",
       "      <td>0.028042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_25</th>\n",
       "      <td>-0.008448</td>\n",
       "      <td>0.031325</td>\n",
       "      <td>-0.046702</td>\n",
       "      <td>-0.004658</td>\n",
       "      <td>0.029590</td>\n",
       "      <td>-0.037439</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>0.043676</td>\n",
       "      <td>0.031322</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010551</td>\n",
       "      <td>-0.017677</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>-0.014182</td>\n",
       "      <td>-0.001704</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>-0.024460</td>\n",
       "      <td>0.003680</td>\n",
       "      <td>0.005815</td>\n",
       "      <td>0.023490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_26</th>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.005006</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>0.046688</td>\n",
       "      <td>0.027534</td>\n",
       "      <td>-0.022852</td>\n",
       "      <td>0.021183</td>\n",
       "      <td>-0.038049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009227</td>\n",
       "      <td>-0.018272</td>\n",
       "      <td>-0.015767</td>\n",
       "      <td>-0.006037</td>\n",
       "      <td>0.011381</td>\n",
       "      <td>0.042520</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>-0.012433</td>\n",
       "      <td>-0.004916</td>\n",
       "      <td>0.036095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_27</th>\n",
       "      <td>-0.016251</td>\n",
       "      <td>0.018352</td>\n",
       "      <td>-0.019346</td>\n",
       "      <td>0.020793</td>\n",
       "      <td>0.011056</td>\n",
       "      <td>-0.042400</td>\n",
       "      <td>-0.027162</td>\n",
       "      <td>-0.045547</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>-0.017160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003213</td>\n",
       "      <td>-0.029978</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.011623</td>\n",
       "      <td>0.021735</td>\n",
       "      <td>0.019496</td>\n",
       "      <td>-0.017074</td>\n",
       "      <td>-0.019755</td>\n",
       "      <td>-0.026212</td>\n",
       "      <td>0.007501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_28</th>\n",
       "      <td>-0.012097</td>\n",
       "      <td>-0.030062</td>\n",
       "      <td>-0.001367</td>\n",
       "      <td>-0.004039</td>\n",
       "      <td>-0.008137</td>\n",
       "      <td>-0.014530</td>\n",
       "      <td>0.034690</td>\n",
       "      <td>0.004542</td>\n",
       "      <td>-0.002321</td>\n",
       "      <td>-0.033860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042977</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>-0.188980</td>\n",
       "      <td>-0.025236</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>0.013264</td>\n",
       "      <td>-0.020745</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>0.046613</td>\n",
       "      <td>0.003571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_29</th>\n",
       "      <td>0.033626</td>\n",
       "      <td>0.014606</td>\n",
       "      <td>0.006957</td>\n",
       "      <td>0.020328</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>-0.004329</td>\n",
       "      <td>-0.001454</td>\n",
       "      <td>-0.018924</td>\n",
       "      <td>-0.007313</td>\n",
       "      <td>-0.006784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037932</td>\n",
       "      <td>0.011573</td>\n",
       "      <td>0.028646</td>\n",
       "      <td>0.032404</td>\n",
       "      <td>0.006469</td>\n",
       "      <td>0.018716</td>\n",
       "      <td>-0.009379</td>\n",
       "      <td>0.033735</td>\n",
       "      <td>0.011742</td>\n",
       "      <td>0.006717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_471</th>\n",
       "      <td>0.034974</td>\n",
       "      <td>0.025280</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.018107</td>\n",
       "      <td>-0.004950</td>\n",
       "      <td>0.035301</td>\n",
       "      <td>0.016165</td>\n",
       "      <td>-0.006011</td>\n",
       "      <td>0.009379</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024062</td>\n",
       "      <td>0.004468</td>\n",
       "      <td>-0.004962</td>\n",
       "      <td>0.017014</td>\n",
       "      <td>0.005914</td>\n",
       "      <td>-0.003869</td>\n",
       "      <td>0.027884</td>\n",
       "      <td>-0.035752</td>\n",
       "      <td>-0.011855</td>\n",
       "      <td>0.036557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_472</th>\n",
       "      <td>0.049647</td>\n",
       "      <td>0.033250</td>\n",
       "      <td>-0.009336</td>\n",
       "      <td>0.031618</td>\n",
       "      <td>-0.001196</td>\n",
       "      <td>0.005237</td>\n",
       "      <td>-0.049729</td>\n",
       "      <td>0.006122</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042099</td>\n",
       "      <td>-0.006405</td>\n",
       "      <td>0.403140</td>\n",
       "      <td>0.026520</td>\n",
       "      <td>-0.015163</td>\n",
       "      <td>-0.013272</td>\n",
       "      <td>0.014343</td>\n",
       "      <td>0.041719</td>\n",
       "      <td>-0.026311</td>\n",
       "      <td>-0.112071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_473</th>\n",
       "      <td>0.002984</td>\n",
       "      <td>-0.006662</td>\n",
       "      <td>0.015692</td>\n",
       "      <td>0.027133</td>\n",
       "      <td>-0.014378</td>\n",
       "      <td>-0.020002</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>-0.033303</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>-0.010768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019568</td>\n",
       "      <td>-0.041969</td>\n",
       "      <td>-0.008119</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>-0.024636</td>\n",
       "      <td>0.024974</td>\n",
       "      <td>-0.004165</td>\n",
       "      <td>-0.021669</td>\n",
       "      <td>-0.011212</td>\n",
       "      <td>0.023220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_474</th>\n",
       "      <td>0.009437</td>\n",
       "      <td>-0.030230</td>\n",
       "      <td>0.032313</td>\n",
       "      <td>0.022565</td>\n",
       "      <td>0.009357</td>\n",
       "      <td>-0.004048</td>\n",
       "      <td>-0.001240</td>\n",
       "      <td>-0.030656</td>\n",
       "      <td>-0.020099</td>\n",
       "      <td>-0.039643</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008091</td>\n",
       "      <td>-0.020121</td>\n",
       "      <td>-0.037700</td>\n",
       "      <td>0.030880</td>\n",
       "      <td>-0.000291</td>\n",
       "      <td>-0.038997</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.009173</td>\n",
       "      <td>0.026658</td>\n",
       "      <td>0.021630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_475</th>\n",
       "      <td>-0.024707</td>\n",
       "      <td>-0.021946</td>\n",
       "      <td>0.005177</td>\n",
       "      <td>-0.023293</td>\n",
       "      <td>0.017467</td>\n",
       "      <td>0.017680</td>\n",
       "      <td>0.024524</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.007600</td>\n",
       "      <td>0.025113</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012313</td>\n",
       "      <td>0.029106</td>\n",
       "      <td>-0.445748</td>\n",
       "      <td>0.011305</td>\n",
       "      <td>0.018347</td>\n",
       "      <td>0.020221</td>\n",
       "      <td>0.024115</td>\n",
       "      <td>-0.009430</td>\n",
       "      <td>-0.018178</td>\n",
       "      <td>0.219933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_476</th>\n",
       "      <td>0.005844</td>\n",
       "      <td>-0.021442</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.003991</td>\n",
       "      <td>-0.027255</td>\n",
       "      <td>-0.007237</td>\n",
       "      <td>-0.003479</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>-0.034021</td>\n",
       "      <td>0.003812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041596</td>\n",
       "      <td>-0.011238</td>\n",
       "      <td>-0.004915</td>\n",
       "      <td>-0.004388</td>\n",
       "      <td>-0.035015</td>\n",
       "      <td>-0.005374</td>\n",
       "      <td>0.013698</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>-0.008051</td>\n",
       "      <td>-0.000503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_477</th>\n",
       "      <td>-0.017426</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.028496</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>-0.029812</td>\n",
       "      <td>0.012805</td>\n",
       "      <td>-0.026051</td>\n",
       "      <td>-0.012110</td>\n",
       "      <td>-0.016841</td>\n",
       "      <td>-0.035786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053899</td>\n",
       "      <td>-0.025257</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>-0.012085</td>\n",
       "      <td>-0.005760</td>\n",
       "      <td>-0.024419</td>\n",
       "      <td>-0.028592</td>\n",
       "      <td>-0.029014</td>\n",
       "      <td>-0.047628</td>\n",
       "      <td>0.014454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_478</th>\n",
       "      <td>0.040020</td>\n",
       "      <td>-0.005211</td>\n",
       "      <td>-0.007584</td>\n",
       "      <td>-0.013685</td>\n",
       "      <td>-0.000907</td>\n",
       "      <td>-0.009964</td>\n",
       "      <td>0.016423</td>\n",
       "      <td>-0.026213</td>\n",
       "      <td>0.007010</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016222</td>\n",
       "      <td>0.030584</td>\n",
       "      <td>0.006981</td>\n",
       "      <td>0.010904</td>\n",
       "      <td>-0.005942</td>\n",
       "      <td>0.024461</td>\n",
       "      <td>-0.020066</td>\n",
       "      <td>0.010792</td>\n",
       "      <td>-0.031897</td>\n",
       "      <td>0.018089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_479</th>\n",
       "      <td>0.021991</td>\n",
       "      <td>-0.005070</td>\n",
       "      <td>0.042801</td>\n",
       "      <td>-0.047111</td>\n",
       "      <td>-0.002926</td>\n",
       "      <td>-0.012571</td>\n",
       "      <td>-0.007254</td>\n",
       "      <td>0.006297</td>\n",
       "      <td>0.015959</td>\n",
       "      <td>-0.021736</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007364</td>\n",
       "      <td>0.019353</td>\n",
       "      <td>0.005816</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>-0.032572</td>\n",
       "      <td>-0.041629</td>\n",
       "      <td>0.019252</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>-0.031246</td>\n",
       "      <td>-0.015308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_480</th>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.037970</td>\n",
       "      <td>-0.010601</td>\n",
       "      <td>-0.004138</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>0.028562</td>\n",
       "      <td>-0.042852</td>\n",
       "      <td>-0.036773</td>\n",
       "      <td>0.014668</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042820</td>\n",
       "      <td>-0.002668</td>\n",
       "      <td>0.021839</td>\n",
       "      <td>-0.012347</td>\n",
       "      <td>-0.027440</td>\n",
       "      <td>-0.002076</td>\n",
       "      <td>-0.007738</td>\n",
       "      <td>0.016070</td>\n",
       "      <td>-0.023670</td>\n",
       "      <td>-0.025385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_481</th>\n",
       "      <td>-0.001403</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.033880</td>\n",
       "      <td>-0.020255</td>\n",
       "      <td>-0.033481</td>\n",
       "      <td>-0.001458</td>\n",
       "      <td>0.019655</td>\n",
       "      <td>0.022755</td>\n",
       "      <td>0.017517</td>\n",
       "      <td>-0.011817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028822</td>\n",
       "      <td>-0.040190</td>\n",
       "      <td>-0.009527</td>\n",
       "      <td>-0.004108</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.002596</td>\n",
       "      <td>-0.023557</td>\n",
       "      <td>-0.010010</td>\n",
       "      <td>0.018627</td>\n",
       "      <td>-0.046721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_482</th>\n",
       "      <td>-0.013659</td>\n",
       "      <td>-0.014423</td>\n",
       "      <td>-0.012498</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.030181</td>\n",
       "      <td>-0.013252</td>\n",
       "      <td>-0.006207</td>\n",
       "      <td>0.021685</td>\n",
       "      <td>-0.001140</td>\n",
       "      <td>-0.033257</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029776</td>\n",
       "      <td>-0.010958</td>\n",
       "      <td>-0.016245</td>\n",
       "      <td>0.030532</td>\n",
       "      <td>-0.009222</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>-0.005280</td>\n",
       "      <td>0.013971</td>\n",
       "      <td>0.010079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_483</th>\n",
       "      <td>-0.003210</td>\n",
       "      <td>-0.003497</td>\n",
       "      <td>-0.002775</td>\n",
       "      <td>0.025885</td>\n",
       "      <td>-0.030435</td>\n",
       "      <td>-0.002118</td>\n",
       "      <td>-0.001142</td>\n",
       "      <td>0.019438</td>\n",
       "      <td>0.030374</td>\n",
       "      <td>0.007050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>0.034632</td>\n",
       "      <td>0.011377</td>\n",
       "      <td>0.006553</td>\n",
       "      <td>-0.018476</td>\n",
       "      <td>-0.021042</td>\n",
       "      <td>0.011818</td>\n",
       "      <td>0.028657</td>\n",
       "      <td>-0.024167</td>\n",
       "      <td>-0.024468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_484</th>\n",
       "      <td>-0.016733</td>\n",
       "      <td>0.028787</td>\n",
       "      <td>0.013430</td>\n",
       "      <td>0.014138</td>\n",
       "      <td>0.039270</td>\n",
       "      <td>0.043053</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>-0.008580</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020217</td>\n",
       "      <td>-0.009168</td>\n",
       "      <td>-0.006678</td>\n",
       "      <td>-0.016562</td>\n",
       "      <td>0.018580</td>\n",
       "      <td>0.011152</td>\n",
       "      <td>-0.051715</td>\n",
       "      <td>0.030034</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.006850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_485</th>\n",
       "      <td>-0.021552</td>\n",
       "      <td>0.006508</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>-0.010758</td>\n",
       "      <td>0.004233</td>\n",
       "      <td>0.012929</td>\n",
       "      <td>-0.026485</td>\n",
       "      <td>-0.020581</td>\n",
       "      <td>-0.010434</td>\n",
       "      <td>-0.004240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006775</td>\n",
       "      <td>-0.009537</td>\n",
       "      <td>-0.011024</td>\n",
       "      <td>-0.028288</td>\n",
       "      <td>-0.003357</td>\n",
       "      <td>-0.009845</td>\n",
       "      <td>0.007756</td>\n",
       "      <td>0.023436</td>\n",
       "      <td>-0.008138</td>\n",
       "      <td>0.005589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_486</th>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.029098</td>\n",
       "      <td>0.011611</td>\n",
       "      <td>-0.019336</td>\n",
       "      <td>0.006955</td>\n",
       "      <td>-0.030466</td>\n",
       "      <td>0.014812</td>\n",
       "      <td>0.025294</td>\n",
       "      <td>0.026340</td>\n",
       "      <td>0.037787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058289</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>-0.000879</td>\n",
       "      <td>0.006846</td>\n",
       "      <td>0.009429</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.020780</td>\n",
       "      <td>0.024690</td>\n",
       "      <td>0.032541</td>\n",
       "      <td>-0.009412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_487</th>\n",
       "      <td>-0.001395</td>\n",
       "      <td>-0.007581</td>\n",
       "      <td>-0.018281</td>\n",
       "      <td>-0.016183</td>\n",
       "      <td>-0.029617</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0.009356</td>\n",
       "      <td>-0.025282</td>\n",
       "      <td>-0.001823</td>\n",
       "      <td>-0.028054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021025</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>0.013556</td>\n",
       "      <td>-0.032960</td>\n",
       "      <td>-0.022601</td>\n",
       "      <td>0.028527</td>\n",
       "      <td>-0.049271</td>\n",
       "      <td>0.026961</td>\n",
       "      <td>-0.020860</td>\n",
       "      <td>-0.003011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_488</th>\n",
       "      <td>-0.024779</td>\n",
       "      <td>-0.016935</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.022259</td>\n",
       "      <td>0.018876</td>\n",
       "      <td>0.024981</td>\n",
       "      <td>-0.009623</td>\n",
       "      <td>-0.009395</td>\n",
       "      <td>0.016488</td>\n",
       "      <td>0.034630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003606</td>\n",
       "      <td>0.004945</td>\n",
       "      <td>0.021948</td>\n",
       "      <td>-0.011217</td>\n",
       "      <td>0.023656</td>\n",
       "      <td>-0.042269</td>\n",
       "      <td>0.009668</td>\n",
       "      <td>-0.031964</td>\n",
       "      <td>-0.045959</td>\n",
       "      <td>-0.002662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_489</th>\n",
       "      <td>-0.015730</td>\n",
       "      <td>-0.016426</td>\n",
       "      <td>-0.014878</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.021118</td>\n",
       "      <td>-0.018168</td>\n",
       "      <td>-0.055484</td>\n",
       "      <td>0.040260</td>\n",
       "      <td>0.025030</td>\n",
       "      <td>-0.025071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015450</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.032339</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.057150</td>\n",
       "      <td>0.003926</td>\n",
       "      <td>-0.001049</td>\n",
       "      <td>0.030870</td>\n",
       "      <td>-0.018632</td>\n",
       "      <td>0.001975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_490</th>\n",
       "      <td>-0.005722</td>\n",
       "      <td>0.039412</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>-0.002917</td>\n",
       "      <td>-0.019680</td>\n",
       "      <td>0.015319</td>\n",
       "      <td>-0.004932</td>\n",
       "      <td>-0.005969</td>\n",
       "      <td>0.017565</td>\n",
       "      <td>-0.015757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018104</td>\n",
       "      <td>-0.010382</td>\n",
       "      <td>0.015330</td>\n",
       "      <td>-0.008254</td>\n",
       "      <td>-0.004765</td>\n",
       "      <td>-0.024198</td>\n",
       "      <td>-0.004043</td>\n",
       "      <td>-0.011802</td>\n",
       "      <td>0.039862</td>\n",
       "      <td>0.027077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_491</th>\n",
       "      <td>0.017542</td>\n",
       "      <td>0.008182</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>-0.045861</td>\n",
       "      <td>0.003109</td>\n",
       "      <td>0.025443</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>-0.024678</td>\n",
       "      <td>-0.002311</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.035868</td>\n",
       "      <td>-0.033220</td>\n",
       "      <td>-0.017902</td>\n",
       "      <td>-0.020613</td>\n",
       "      <td>-0.030389</td>\n",
       "      <td>0.003526</td>\n",
       "      <td>0.003615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_492</th>\n",
       "      <td>-0.003865</td>\n",
       "      <td>0.018399</td>\n",
       "      <td>-0.000458</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>-0.039532</td>\n",
       "      <td>-0.024741</td>\n",
       "      <td>0.013658</td>\n",
       "      <td>0.021953</td>\n",
       "      <td>0.008186</td>\n",
       "      <td>0.042103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.016965</td>\n",
       "      <td>-0.005374</td>\n",
       "      <td>-0.015171</td>\n",
       "      <td>0.007196</td>\n",
       "      <td>0.020319</td>\n",
       "      <td>0.009221</td>\n",
       "      <td>-0.014817</td>\n",
       "      <td>-0.012082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_493</th>\n",
       "      <td>0.015519</td>\n",
       "      <td>0.026583</td>\n",
       "      <td>-0.013680</td>\n",
       "      <td>0.017454</td>\n",
       "      <td>-0.013767</td>\n",
       "      <td>-0.016164</td>\n",
       "      <td>-0.015206</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.027587</td>\n",
       "      <td>-0.033364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>-0.016965</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.003169</td>\n",
       "      <td>-0.043973</td>\n",
       "      <td>-0.041137</td>\n",
       "      <td>0.020590</td>\n",
       "      <td>-0.013937</td>\n",
       "      <td>-0.095056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_494</th>\n",
       "      <td>0.007361</td>\n",
       "      <td>-0.002381</td>\n",
       "      <td>0.009596</td>\n",
       "      <td>-0.007225</td>\n",
       "      <td>0.031348</td>\n",
       "      <td>-0.032220</td>\n",
       "      <td>0.019108</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>-0.013677</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035868</td>\n",
       "      <td>-0.005374</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041158</td>\n",
       "      <td>-0.009153</td>\n",
       "      <td>-0.029623</td>\n",
       "      <td>-0.008602</td>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.045805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_495</th>\n",
       "      <td>-0.034734</td>\n",
       "      <td>0.018728</td>\n",
       "      <td>0.026388</td>\n",
       "      <td>0.013877</td>\n",
       "      <td>0.028628</td>\n",
       "      <td>0.028203</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>0.009076</td>\n",
       "      <td>-0.008730</td>\n",
       "      <td>-0.022317</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033220</td>\n",
       "      <td>-0.015171</td>\n",
       "      <td>0.003169</td>\n",
       "      <td>0.041158</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002494</td>\n",
       "      <td>-0.021698</td>\n",
       "      <td>-0.030119</td>\n",
       "      <td>-0.012242</td>\n",
       "      <td>-0.006061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_496</th>\n",
       "      <td>0.013165</td>\n",
       "      <td>0.043230</td>\n",
       "      <td>0.010825</td>\n",
       "      <td>-0.010965</td>\n",
       "      <td>0.053824</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.031925</td>\n",
       "      <td>0.016342</td>\n",
       "      <td>-0.032714</td>\n",
       "      <td>0.073471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017902</td>\n",
       "      <td>0.007196</td>\n",
       "      <td>-0.043973</td>\n",
       "      <td>-0.009153</td>\n",
       "      <td>-0.002494</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009619</td>\n",
       "      <td>-0.002916</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>-0.053324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_497</th>\n",
       "      <td>-0.033386</td>\n",
       "      <td>-0.006259</td>\n",
       "      <td>0.037516</td>\n",
       "      <td>-0.003627</td>\n",
       "      <td>0.012479</td>\n",
       "      <td>0.010998</td>\n",
       "      <td>-0.027418</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>-0.009209</td>\n",
       "      <td>-0.001086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020613</td>\n",
       "      <td>0.020319</td>\n",
       "      <td>-0.041137</td>\n",
       "      <td>-0.029623</td>\n",
       "      <td>-0.021698</td>\n",
       "      <td>0.009619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012177</td>\n",
       "      <td>0.028701</td>\n",
       "      <td>0.036145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_498</th>\n",
       "      <td>0.008648</td>\n",
       "      <td>-0.001824</td>\n",
       "      <td>0.022287</td>\n",
       "      <td>-0.014491</td>\n",
       "      <td>0.026136</td>\n",
       "      <td>-0.016103</td>\n",
       "      <td>0.026832</td>\n",
       "      <td>0.008581</td>\n",
       "      <td>0.020083</td>\n",
       "      <td>0.010612</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030389</td>\n",
       "      <td>0.009221</td>\n",
       "      <td>0.020590</td>\n",
       "      <td>-0.008602</td>\n",
       "      <td>-0.030119</td>\n",
       "      <td>-0.002916</td>\n",
       "      <td>-0.012177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013563</td>\n",
       "      <td>0.002203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_499</th>\n",
       "      <td>-0.046535</td>\n",
       "      <td>0.041259</td>\n",
       "      <td>0.020370</td>\n",
       "      <td>-0.000589</td>\n",
       "      <td>0.010764</td>\n",
       "      <td>0.017191</td>\n",
       "      <td>0.016975</td>\n",
       "      <td>0.014249</td>\n",
       "      <td>0.034521</td>\n",
       "      <td>-0.047127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003526</td>\n",
       "      <td>-0.014817</td>\n",
       "      <td>-0.013937</td>\n",
       "      <td>0.015228</td>\n",
       "      <td>-0.012242</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.028701</td>\n",
       "      <td>0.013563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.019003</td>\n",
       "      <td>0.011906</td>\n",
       "      <td>-0.002153</td>\n",
       "      <td>0.042497</td>\n",
       "      <td>-0.031056</td>\n",
       "      <td>-0.016405</td>\n",
       "      <td>0.015172</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>-0.009459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003615</td>\n",
       "      <td>-0.012082</td>\n",
       "      <td>-0.095056</td>\n",
       "      <td>0.045805</td>\n",
       "      <td>-0.006061</td>\n",
       "      <td>-0.053324</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            feat_0    feat_1    feat_2    feat_3    feat_4    feat_5  \\\n",
       "feat_0    1.000000  0.042789 -0.022897  0.017871 -0.003337  0.017776   \n",
       "feat_1    0.042789  1.000000 -0.008173  0.020494  0.029011 -0.014954   \n",
       "feat_2   -0.022897 -0.008173  1.000000 -0.009640  0.007198  0.016953   \n",
       "feat_3    0.017871  0.020494 -0.009640  1.000000 -0.010786  0.044198   \n",
       "feat_4   -0.003337  0.029011  0.007198 -0.010786  1.000000  0.005714   \n",
       "feat_5    0.017776 -0.014954  0.016953  0.044198  0.005714  1.000000   \n",
       "feat_6    0.036649 -0.032232  0.004855 -0.009176  0.009717 -0.047054   \n",
       "feat_7    0.046846  0.020178 -0.018082 -0.017608  0.024661 -0.024812   \n",
       "feat_8   -0.024170  0.015920 -0.004728 -0.035731 -0.058043  0.042148   \n",
       "feat_9   -0.012535  0.001608 -0.018773  0.010161 -0.027877  0.022175   \n",
       "feat_10  -0.028289 -0.008287  0.011180 -0.007691 -0.035444  0.006439   \n",
       "feat_11   0.046545 -0.013747  0.019508 -0.007620  0.044365  0.022871   \n",
       "feat_12   0.035533 -0.016667 -0.025514  0.001211  0.013538 -0.013960   \n",
       "feat_13   0.035364 -0.002263 -0.004344 -0.013538  0.000983  0.004467   \n",
       "feat_14  -0.002309 -0.015934  0.016829 -0.006832 -0.048193  0.020685   \n",
       "feat_15   0.022011 -0.006341  0.022330 -0.031577 -0.004214 -0.003504   \n",
       "feat_16  -0.017550 -0.001521 -0.006896  0.048102  0.006396  0.033854   \n",
       "feat_17  -0.023106  0.006835 -0.018727 -0.034937  0.008656 -0.023041   \n",
       "feat_18  -0.035264 -0.007024  0.010042  0.013371 -0.002458  0.024916   \n",
       "feat_19   0.033320 -0.004546 -0.001934 -0.035104 -0.008122 -0.019208   \n",
       "feat_20   0.011568 -0.050863  0.014647  0.007314  0.008524 -0.002222   \n",
       "feat_21  -0.011032  0.002452  0.029135 -0.018933  0.034279  0.007700   \n",
       "feat_22  -0.011923 -0.017111 -0.003161  0.031944 -0.009345 -0.031033   \n",
       "feat_23   0.013634  0.037519  0.041670 -0.043839  0.007481 -0.043683   \n",
       "feat_24  -0.020635  0.003243 -0.019119  0.039047  0.010471 -0.028574   \n",
       "feat_25  -0.008448  0.031325 -0.046702 -0.004658  0.029590 -0.037439   \n",
       "feat_26   0.002628  0.005006  0.001301  0.001212  0.003768  0.046688   \n",
       "feat_27  -0.016251  0.018352 -0.019346  0.020793  0.011056 -0.042400   \n",
       "feat_28  -0.012097 -0.030062 -0.001367 -0.004039 -0.008137 -0.014530   \n",
       "feat_29   0.033626  0.014606  0.006957  0.020328  0.011846 -0.004329   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "feat_471  0.034974  0.025280  0.002522  0.018107 -0.004950  0.035301   \n",
       "feat_472  0.049647  0.033250 -0.009336  0.031618 -0.001196  0.005237   \n",
       "feat_473  0.002984 -0.006662  0.015692  0.027133 -0.014378 -0.020002   \n",
       "feat_474  0.009437 -0.030230  0.032313  0.022565  0.009357 -0.004048   \n",
       "feat_475 -0.024707 -0.021946  0.005177 -0.023293  0.017467  0.017680   \n",
       "feat_476  0.005844 -0.021442  0.011300  0.003991 -0.027255 -0.007237   \n",
       "feat_477 -0.017426  0.000338  0.028496  0.015873 -0.029812  0.012805   \n",
       "feat_478  0.040020 -0.005211 -0.007584 -0.013685 -0.000907 -0.009964   \n",
       "feat_479  0.021991 -0.005070  0.042801 -0.047111 -0.002926 -0.012571   \n",
       "feat_480  0.000033  0.037970 -0.010601 -0.004138  0.003921  0.028562   \n",
       "feat_481 -0.001403  0.002551  0.033880 -0.020255 -0.033481 -0.001458   \n",
       "feat_482 -0.013659 -0.014423 -0.012498  0.001597  0.030181 -0.013252   \n",
       "feat_483 -0.003210 -0.003497 -0.002775  0.025885 -0.030435 -0.002118   \n",
       "feat_484 -0.016733  0.028787  0.013430  0.014138  0.039270  0.043053   \n",
       "feat_485 -0.021552  0.006508  0.016428 -0.010758  0.004233  0.012929   \n",
       "feat_486  0.011314  0.029098  0.011611 -0.019336  0.006955 -0.030466   \n",
       "feat_487 -0.001395 -0.007581 -0.018281 -0.016183 -0.029617  0.003863   \n",
       "feat_488 -0.024779 -0.016935  0.001614  0.022259  0.018876  0.024981   \n",
       "feat_489 -0.015730 -0.016426 -0.014878  0.002772  0.021118 -0.018168   \n",
       "feat_490 -0.005722  0.039412  0.001093 -0.002917 -0.019680  0.015319   \n",
       "feat_491  0.017542  0.008182  0.004179 -0.045861  0.003109  0.025443   \n",
       "feat_492 -0.003865  0.018399 -0.000458 -0.030744 -0.039532 -0.024741   \n",
       "feat_493  0.015519  0.026583 -0.013680  0.017454 -0.013767 -0.016164   \n",
       "feat_494  0.007361 -0.002381  0.009596 -0.007225  0.031348 -0.032220   \n",
       "feat_495 -0.034734  0.018728  0.026388  0.013877  0.028628  0.028203   \n",
       "feat_496  0.013165  0.043230  0.010825 -0.010965  0.053824  0.003333   \n",
       "feat_497 -0.033386 -0.006259  0.037516 -0.003627  0.012479  0.010998   \n",
       "feat_498  0.008648 -0.001824  0.022287 -0.014491  0.026136 -0.016103   \n",
       "feat_499 -0.046535  0.041259  0.020370 -0.000589  0.010764  0.017191   \n",
       "labels    0.002414  0.019003  0.011906 -0.002153  0.042497 -0.031056   \n",
       "\n",
       "            feat_6    feat_7    feat_8    feat_9  ...  feat_491  feat_492  \\\n",
       "feat_0    0.036649  0.046846 -0.024170 -0.012535  ...  0.017542 -0.003865   \n",
       "feat_1   -0.032232  0.020178  0.015920  0.001608  ...  0.008182  0.018399   \n",
       "feat_2    0.004855 -0.018082 -0.004728 -0.018773  ...  0.004179 -0.000458   \n",
       "feat_3   -0.009176 -0.017608 -0.035731  0.010161  ... -0.045861 -0.030744   \n",
       "feat_4    0.009717  0.024661 -0.058043 -0.027877  ...  0.003109 -0.039532   \n",
       "feat_5   -0.047054 -0.024812  0.042148  0.022175  ...  0.025443 -0.024741   \n",
       "feat_6    1.000000  0.026023 -0.008711 -0.024414  ...  0.000136  0.013658   \n",
       "feat_7    0.026023  1.000000 -0.035779  0.016051  ... -0.024678  0.021953   \n",
       "feat_8   -0.008711 -0.035779  1.000000  0.002039  ... -0.002311  0.008186   \n",
       "feat_9   -0.024414  0.016051  0.002039  1.000000  ...  0.000077  0.042103   \n",
       "feat_10  -0.016498  0.024683  0.000723 -0.023693  ... -0.005208  0.008730   \n",
       "feat_11   0.008126 -0.032809 -0.034526  0.025780  ...  0.030249 -0.040590   \n",
       "feat_12   0.015700  0.000170  0.018793 -0.026403  ...  0.006102 -0.000917   \n",
       "feat_13   0.011463  0.030127  0.017857  0.022669  ...  0.009394  0.026111   \n",
       "feat_14   0.016099 -0.010436 -0.035499  0.014064  ...  0.022314  0.022235   \n",
       "feat_15   0.021716  0.021439 -0.016426 -0.003054  ...  0.007723 -0.007459   \n",
       "feat_16  -0.030382 -0.006547 -0.010707  0.011346  ...  0.003646 -0.028106   \n",
       "feat_17   0.009720 -0.013263  0.006774 -0.002309  ...  0.021513 -0.017691   \n",
       "feat_18   0.062293 -0.026648 -0.011219  0.051074  ...  0.031787 -0.008926   \n",
       "feat_19  -0.011885 -0.047448 -0.019565 -0.016438  ... -0.021215 -0.010067   \n",
       "feat_20  -0.035338 -0.041613 -0.008407  0.006416  ...  0.004531 -0.013069   \n",
       "feat_21   0.002710  0.009685 -0.008820  0.009271  ... -0.009258  0.012236   \n",
       "feat_22  -0.004462  0.003513 -0.006769 -0.023220  ... -0.012487 -0.052445   \n",
       "feat_23   0.006662  0.033601 -0.012514  0.034675  ... -0.011107  0.020871   \n",
       "feat_24   0.029267  0.006652  0.015089 -0.052089  ... -0.006032  0.010320   \n",
       "feat_25  -0.007812  0.043676  0.031322  0.003493  ... -0.010551 -0.017677   \n",
       "feat_26   0.027534 -0.022852  0.021183 -0.038049  ...  0.009227 -0.018272   \n",
       "feat_27  -0.027162 -0.045547  0.000092 -0.017160  ...  0.003213 -0.029978   \n",
       "feat_28   0.034690  0.004542 -0.002321 -0.033860  ... -0.042977  0.001503   \n",
       "feat_29  -0.001454 -0.018924 -0.007313 -0.006784  ...  0.037932  0.011573   \n",
       "...            ...       ...       ...       ...  ...       ...       ...   \n",
       "feat_471  0.016165 -0.006011  0.009379  0.009657  ...  0.024062  0.004468   \n",
       "feat_472 -0.049729  0.006122  0.015700  0.002698  ...  0.042099 -0.006405   \n",
       "feat_473  0.002253 -0.033303  0.003246 -0.010768  ... -0.019568 -0.041969   \n",
       "feat_474 -0.001240 -0.030656 -0.020099 -0.039643  ... -0.008091 -0.020121   \n",
       "feat_475  0.024524 -0.000079 -0.007600  0.025113  ... -0.012313  0.029106   \n",
       "feat_476 -0.003479  0.001441 -0.034021  0.003812  ...  0.041596 -0.011238   \n",
       "feat_477 -0.026051 -0.012110 -0.016841 -0.035786  ...  0.053899 -0.025257   \n",
       "feat_478  0.016423 -0.026213  0.007010  0.003998  ...  0.016222  0.030584   \n",
       "feat_479 -0.007254  0.006297  0.015959 -0.021736  ... -0.007364  0.019353   \n",
       "feat_480 -0.042852 -0.036773  0.014668  0.005600  ...  0.042820 -0.002668   \n",
       "feat_481  0.019655  0.022755  0.017517 -0.011817  ...  0.028822 -0.040190   \n",
       "feat_482 -0.006207  0.021685 -0.001140 -0.033257  ... -0.029776 -0.010958   \n",
       "feat_483 -0.001142  0.019438  0.030374  0.007050  ... -0.000864  0.034632   \n",
       "feat_484  0.023768 -0.008580  0.017708  0.000494  ... -0.020217 -0.009168   \n",
       "feat_485 -0.026485 -0.020581 -0.010434 -0.004240  ...  0.006775 -0.009537   \n",
       "feat_486  0.014812  0.025294  0.026340  0.037787  ... -0.058289  0.002513   \n",
       "feat_487  0.009356 -0.025282 -0.001823 -0.028054  ... -0.021025  0.002425   \n",
       "feat_488 -0.009623 -0.009395  0.016488  0.034630  ... -0.003606  0.004945   \n",
       "feat_489 -0.055484  0.040260  0.025030 -0.025071  ...  0.015450  0.000207   \n",
       "feat_490 -0.004932 -0.005969  0.017565 -0.015757  ...  0.018104 -0.010382   \n",
       "feat_491  0.000136 -0.024678 -0.002311  0.000077  ...  1.000000  0.004570   \n",
       "feat_492  0.013658  0.021953  0.008186  0.042103  ...  0.004570  1.000000   \n",
       "feat_493 -0.015206  0.008258  0.027587 -0.033364  ...  0.001106 -0.016965   \n",
       "feat_494  0.019108  0.000431 -0.013677  0.001788  ...  0.035868 -0.005374   \n",
       "feat_495  0.002374  0.009076 -0.008730 -0.022317  ... -0.033220 -0.015171   \n",
       "feat_496  0.031925  0.016342 -0.032714  0.073471  ... -0.017902  0.007196   \n",
       "feat_497 -0.027418  0.004899 -0.009209 -0.001086  ... -0.020613  0.020319   \n",
       "feat_498  0.026832  0.008581  0.020083  0.010612  ... -0.030389  0.009221   \n",
       "feat_499  0.016975  0.014249  0.034521 -0.047127  ...  0.003526 -0.014817   \n",
       "labels   -0.016405  0.015172  0.009541 -0.009459  ...  0.003615 -0.012082   \n",
       "\n",
       "          feat_493  feat_494  feat_495  feat_496  feat_497  feat_498  \\\n",
       "feat_0    0.015519  0.007361 -0.034734  0.013165 -0.033386  0.008648   \n",
       "feat_1    0.026583 -0.002381  0.018728  0.043230 -0.006259 -0.001824   \n",
       "feat_2   -0.013680  0.009596  0.026388  0.010825  0.037516  0.022287   \n",
       "feat_3    0.017454 -0.007225  0.013877 -0.010965 -0.003627 -0.014491   \n",
       "feat_4   -0.013767  0.031348  0.028628  0.053824  0.012479  0.026136   \n",
       "feat_5   -0.016164 -0.032220  0.028203  0.003333  0.010998 -0.016103   \n",
       "feat_6   -0.015206  0.019108  0.002374  0.031925 -0.027418  0.026832   \n",
       "feat_7    0.008258  0.000431  0.009076  0.016342  0.004899  0.008581   \n",
       "feat_8    0.027587 -0.013677 -0.008730 -0.032714 -0.009209  0.020083   \n",
       "feat_9   -0.033364  0.001788 -0.022317  0.073471 -0.001086  0.010612   \n",
       "feat_10   0.001386 -0.022679  0.021158 -0.005951 -0.014404  0.028908   \n",
       "feat_11   0.012397  0.020546  0.039812  0.010577  0.028644  0.010386   \n",
       "feat_12   0.009567  0.046594  0.021604  0.017387 -0.021652  0.036468   \n",
       "feat_13  -0.006425 -0.010519  0.036022 -0.002835 -0.007523  0.058581   \n",
       "feat_14  -0.007420  0.021198 -0.009774 -0.003077  0.004001  0.030357   \n",
       "feat_15   0.008166 -0.016099  0.062253  0.010355 -0.006185 -0.040172   \n",
       "feat_16  -0.011146  0.037487 -0.027471  0.036239 -0.018152  0.000826   \n",
       "feat_17  -0.008233 -0.009109  0.015044  0.032901  0.008614  0.001216   \n",
       "feat_18  -0.035670 -0.015116  0.016753  0.002814 -0.009429 -0.031442   \n",
       "feat_19   0.013748  0.001330 -0.013806  0.017365  0.025462  0.003262   \n",
       "feat_20   0.025213  0.015819  0.015118  0.018748 -0.031236 -0.002514   \n",
       "feat_21  -0.000703 -0.028971  0.031428  0.001546 -0.030445  0.017736   \n",
       "feat_22   0.007643 -0.018234 -0.013856  0.016894 -0.030705  0.010185   \n",
       "feat_23   0.016665  0.011878  0.018134 -0.016175 -0.003524 -0.020215   \n",
       "feat_24   0.018516  0.029944  0.018211 -0.004224 -0.011241 -0.020421   \n",
       "feat_25   0.000467 -0.014182 -0.001704  0.029800 -0.024460  0.003680   \n",
       "feat_26  -0.015767 -0.006037  0.011381  0.042520  0.005262 -0.012433   \n",
       "feat_27   0.006830  0.011623  0.021735  0.019496 -0.017074 -0.019755   \n",
       "feat_28  -0.188980 -0.025236  0.004485  0.013264 -0.020745 -0.040009   \n",
       "feat_29   0.028646  0.032404  0.006469  0.018716 -0.009379  0.033735   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "feat_471 -0.004962  0.017014  0.005914 -0.003869  0.027884 -0.035752   \n",
       "feat_472  0.403140  0.026520 -0.015163 -0.013272  0.014343  0.041719   \n",
       "feat_473 -0.008119  0.004092 -0.024636  0.024974 -0.004165 -0.021669   \n",
       "feat_474 -0.037700  0.030880 -0.000291 -0.038997  0.020846  0.009173   \n",
       "feat_475 -0.445748  0.011305  0.018347  0.020221  0.024115 -0.009430   \n",
       "feat_476 -0.004915 -0.004388 -0.035015 -0.005374  0.013698  0.011800   \n",
       "feat_477  0.002584 -0.012085 -0.005760 -0.024419 -0.028592 -0.029014   \n",
       "feat_478  0.006981  0.010904 -0.005942  0.024461 -0.020066  0.010792   \n",
       "feat_479  0.005816  0.001067 -0.032572 -0.041629  0.019252  0.003593   \n",
       "feat_480  0.021839 -0.012347 -0.027440 -0.002076 -0.007738  0.016070   \n",
       "feat_481 -0.009527 -0.004108  0.001018  0.002596 -0.023557 -0.010010   \n",
       "feat_482 -0.016245  0.030532 -0.009222 -0.015972  0.002801 -0.005280   \n",
       "feat_483  0.011377  0.006553 -0.018476 -0.021042  0.011818  0.028657   \n",
       "feat_484 -0.006678 -0.016562  0.018580  0.011152 -0.051715  0.030034   \n",
       "feat_485 -0.011024 -0.028288 -0.003357 -0.009845  0.007756  0.023436   \n",
       "feat_486 -0.000879  0.006846  0.009429  0.020542  0.020780  0.024690   \n",
       "feat_487  0.013556 -0.032960 -0.022601  0.028527 -0.049271  0.026961   \n",
       "feat_488  0.021948 -0.011217  0.023656 -0.042269  0.009668 -0.031964   \n",
       "feat_489  0.032339  0.000225  0.057150  0.003926 -0.001049  0.030870   \n",
       "feat_490  0.015330 -0.008254 -0.004765 -0.024198 -0.004043 -0.011802   \n",
       "feat_491  0.001106  0.035868 -0.033220 -0.017902 -0.020613 -0.030389   \n",
       "feat_492 -0.016965 -0.005374 -0.015171  0.007196  0.020319  0.009221   \n",
       "feat_493  1.000000  0.003024  0.003169 -0.043973 -0.041137  0.020590   \n",
       "feat_494  0.003024  1.000000  0.041158 -0.009153 -0.029623 -0.008602   \n",
       "feat_495  0.003169  0.041158  1.000000 -0.002494 -0.021698 -0.030119   \n",
       "feat_496 -0.043973 -0.009153 -0.002494  1.000000  0.009619 -0.002916   \n",
       "feat_497 -0.041137 -0.029623 -0.021698  0.009619  1.000000 -0.012177   \n",
       "feat_498  0.020590 -0.008602 -0.030119 -0.002916 -0.012177  1.000000   \n",
       "feat_499 -0.013937  0.015228 -0.012242  0.000442  0.028701  0.013563   \n",
       "labels   -0.095056  0.045805 -0.006061 -0.053324  0.036145  0.002203   \n",
       "\n",
       "          feat_499    labels  \n",
       "feat_0   -0.046535  0.002414  \n",
       "feat_1    0.041259  0.019003  \n",
       "feat_2    0.020370  0.011906  \n",
       "feat_3   -0.000589 -0.002153  \n",
       "feat_4    0.010764  0.042497  \n",
       "feat_5    0.017191 -0.031056  \n",
       "feat_6    0.016975 -0.016405  \n",
       "feat_7    0.014249  0.015172  \n",
       "feat_8    0.034521  0.009541  \n",
       "feat_9   -0.047127 -0.009459  \n",
       "feat_10  -0.026111 -0.048361  \n",
       "feat_11  -0.040684  0.005303  \n",
       "feat_12  -0.048082  0.031893  \n",
       "feat_13  -0.032736 -0.011137  \n",
       "feat_14   0.002941 -0.023038  \n",
       "feat_15  -0.045842  0.015765  \n",
       "feat_16   0.006333 -0.016765  \n",
       "feat_17  -0.023839 -0.006149  \n",
       "feat_18   0.012699  0.031288  \n",
       "feat_19   0.005229 -0.027223  \n",
       "feat_20  -0.014854 -0.014661  \n",
       "feat_21  -0.014065  0.011415  \n",
       "feat_22  -0.016642  0.009651  \n",
       "feat_23  -0.029700 -0.020800  \n",
       "feat_24  -0.018546  0.028042  \n",
       "feat_25   0.005815  0.023490  \n",
       "feat_26  -0.004916  0.036095  \n",
       "feat_27  -0.026212  0.007501  \n",
       "feat_28   0.046613  0.003571  \n",
       "feat_29   0.011742  0.006717  \n",
       "...            ...       ...  \n",
       "feat_471 -0.011855  0.036557  \n",
       "feat_472 -0.026311 -0.112071  \n",
       "feat_473 -0.011212  0.023220  \n",
       "feat_474  0.026658  0.021630  \n",
       "feat_475 -0.018178  0.219933  \n",
       "feat_476 -0.008051 -0.000503  \n",
       "feat_477 -0.047628  0.014454  \n",
       "feat_478 -0.031897  0.018089  \n",
       "feat_479 -0.031246 -0.015308  \n",
       "feat_480 -0.023670 -0.025385  \n",
       "feat_481  0.018627 -0.046721  \n",
       "feat_482  0.013971  0.010079  \n",
       "feat_483 -0.024167 -0.024468  \n",
       "feat_484  0.020690  0.006850  \n",
       "feat_485 -0.008138  0.005589  \n",
       "feat_486  0.032541 -0.009412  \n",
       "feat_487 -0.020860 -0.003011  \n",
       "feat_488 -0.045959 -0.002662  \n",
       "feat_489 -0.018632  0.001975  \n",
       "feat_490  0.039862  0.027077  \n",
       "feat_491  0.003526  0.003615  \n",
       "feat_492 -0.014817 -0.012082  \n",
       "feat_493 -0.013937 -0.095056  \n",
       "feat_494  0.015228  0.045805  \n",
       "feat_495 -0.012242 -0.006061  \n",
       "feat_496  0.000442 -0.053324  \n",
       "feat_497  0.028701  0.036145  \n",
       "feat_498  0.013563  0.002203  \n",
       "feat_499  1.000000  0.001549  \n",
       "labels    0.001549  1.000000  \n",
       "\n",
       "[501 rows x 501 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 500)\n",
      "(1000, 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2540beb6a58>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train=data.drop(columns='labels',axis=0).values\n",
    "Y_train=data['labels'].values\n",
    "X_test=test_data.drop(columns='labels',axis=0).values\n",
    "Y_test=test_data['labels'].values\n",
    "\n",
    "\n",
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)\n",
    "\n",
    "\n",
    "#PCA for Data Visualization\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "positive_samples=data.where(data['labels']==1).dropna()\n",
    "negative_samples=data.where(data['labels']==0).dropna()\n",
    "positive_samples=positive_samples.drop(columns='labels',axis=0)\n",
    "negative_samples=negative_samples.drop(columns='labels',axis=0)\n",
    "\n",
    "\n",
    "print(positive_samples.shape)\n",
    "print(negative_samples.shape)\n",
    "pca=PCA(n_components=2)\n",
    "PCA_visualize=pca.fit(X_train)\n",
    "positive_samples=PCA_visualize.transform(positive_samples)\n",
    "negative_samples=PCA_visualize.transform(negative_samples)\n",
    "#X_test=pca.transform(X_test)\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(positive_samples[:,0],positive_samples[:,1],color='b')\n",
    "plt.scatter(negative_samples[:,0],negative_samples[:,1],color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though only a portion of variance is captured by the principal components of the training set, we can expect that the data is not linearly seperable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "55slmaFsdvLu"
   },
   "source": [
    "Task 2: Use scikit-learn's logistic regression to establish a baseline model. We are not expecting this model to perform well, we just want to know what is the highest accuracy that we can achieve without doing any feature engineering and/or parameter tuning. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_qi7_QMeH4M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.7405\n",
      "Test Accuracy 0.58\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,Y_train)\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_test_pred=model.predict(X_test)\n",
    "print(\"Train Accuracy\",accuracy_score(y_train_pred,Y_train))\n",
    "print(\"Test Accuracy\",accuracy_score(y_test_pred,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.524\n",
      "0.5066666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2540c282d68>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "training_scores=[]\n",
    "test_scores=[]\n",
    "r_lambda=[0.1,0.5,1.5,3,3.5,5,8,10,15,20,50,100,1000]\n",
    "for i in r_lambda:\n",
    "  classifier=LogisticRegression(penalty='l2',C=1/i)\n",
    "  classifier.fit(X_train,Y_train)\n",
    "  training_prediction=classifier.predict(X_train)\n",
    "  test_prediction=classifier.predict(X_test)\n",
    "  training_scores.append(accuracy_score(training_prediction,Y_train))\n",
    "  test_scores.append(accuracy_score(test_prediction,Y_test))\n",
    "\n",
    "print(max(training_scores))\n",
    "print(max(test_scores))\n",
    "\n",
    "plt.plot(r_lambda,training_scores,\"b\",label=\"training accuracy\")\n",
    "plt.plot(r_lambda,test_scores,\"r\",label=\"test accuracy\")\n",
    "plt.xlabel(\"Lambda λ\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xPtBfYieKt7"
   },
   "source": [
    "Task 3: Feature engineering. On this task you'll write code to modify your features such that the model can achieve a higher accuracy. You are free to modify the data as you want, but a rationale has to be provided for each modification. Additionally, if you are using anything that has not been convered in class, write a brief description of how the function works. Note that copy/pasting a function's docstring is not what we are asking and we will take off points if this is what you do. (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt  \n",
    "import seaborn as sns\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>423</td>\n",
       "      <td>504</td>\n",
       "      <td>493</td>\n",
       "      <td>521</td>\n",
       "      <td>466</td>\n",
       "      <td>494</td>\n",
       "      <td>479</td>\n",
       "      <td>482</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>478</td>\n",
       "      <td>479</td>\n",
       "      <td>567</td>\n",
       "      <td>547</td>\n",
       "      <td>498</td>\n",
       "      <td>484</td>\n",
       "      <td>474</td>\n",
       "      <td>567</td>\n",
       "      <td>538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483</td>\n",
       "      <td>499</td>\n",
       "      <td>520</td>\n",
       "      <td>467</td>\n",
       "      <td>495</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>477</td>\n",
       "      <td>488</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>481</td>\n",
       "      <td>484</td>\n",
       "      <td>451</td>\n",
       "      <td>445</td>\n",
       "      <td>443</td>\n",
       "      <td>481</td>\n",
       "      <td>485</td>\n",
       "      <td>492</td>\n",
       "      <td>477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>487</td>\n",
       "      <td>486</td>\n",
       "      <td>495</td>\n",
       "      <td>481</td>\n",
       "      <td>421</td>\n",
       "      <td>481</td>\n",
       "      <td>499</td>\n",
       "      <td>478</td>\n",
       "      <td>489</td>\n",
       "      <td>482</td>\n",
       "      <td>...</td>\n",
       "      <td>477</td>\n",
       "      <td>511</td>\n",
       "      <td>245</td>\n",
       "      <td>522</td>\n",
       "      <td>480</td>\n",
       "      <td>483</td>\n",
       "      <td>493</td>\n",
       "      <td>421</td>\n",
       "      <td>488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480</td>\n",
       "      <td>427</td>\n",
       "      <td>531</td>\n",
       "      <td>458</td>\n",
       "      <td>544</td>\n",
       "      <td>492</td>\n",
       "      <td>489</td>\n",
       "      <td>477</td>\n",
       "      <td>478</td>\n",
       "      <td>482</td>\n",
       "      <td>...</td>\n",
       "      <td>483</td>\n",
       "      <td>471</td>\n",
       "      <td>313</td>\n",
       "      <td>490</td>\n",
       "      <td>414</td>\n",
       "      <td>480</td>\n",
       "      <td>516</td>\n",
       "      <td>495</td>\n",
       "      <td>469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>491</td>\n",
       "      <td>472</td>\n",
       "      <td>430</td>\n",
       "      <td>463</td>\n",
       "      <td>431</td>\n",
       "      <td>480</td>\n",
       "      <td>459</td>\n",
       "      <td>477</td>\n",
       "      <td>481</td>\n",
       "      <td>479</td>\n",
       "      <td>...</td>\n",
       "      <td>479</td>\n",
       "      <td>493</td>\n",
       "      <td>435</td>\n",
       "      <td>444</td>\n",
       "      <td>455</td>\n",
       "      <td>482</td>\n",
       "      <td>468</td>\n",
       "      <td>497</td>\n",
       "      <td>435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0     485     423     504     493     521     466     494     479     482   \n",
       "1     483     499     520     467     495     484     485     477     488   \n",
       "2     487     486     495     481     421     481     499     478     489   \n",
       "3     480     427     531     458     544     492     489     477     478   \n",
       "4     491     472     430     463     431     480     459     477     481   \n",
       "\n",
       "   feat_9  ...  feat_491  feat_492  feat_493  feat_494  feat_495  feat_496  \\\n",
       "0     471  ...       478       479       567       547       498       484   \n",
       "1     491  ...       481       484       451       445       443       481   \n",
       "2     482  ...       477       511       245       522       480       483   \n",
       "3     482  ...       483       471       313       490       414       480   \n",
       "4     479  ...       479       493       435       444       455       482   \n",
       "\n",
       "   feat_497  feat_498  feat_499  labels  \n",
       "0       474       567       538       0  \n",
       "1       485       492       477       1  \n",
       "2       493       421       488       1  \n",
       "3       516       495       469       0  \n",
       "4       468       497       435       1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('a4-train.csv', index_col=0)\n",
    "test_data=pd.read_csv('a4-test.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>486</td>\n",
       "      <td>497</td>\n",
       "      <td>494</td>\n",
       "      <td>477</td>\n",
       "      <td>582</td>\n",
       "      <td>478</td>\n",
       "      <td>535</td>\n",
       "      <td>477</td>\n",
       "      <td>496</td>\n",
       "      <td>480</td>\n",
       "      <td>...</td>\n",
       "      <td>485</td>\n",
       "      <td>473</td>\n",
       "      <td>576</td>\n",
       "      <td>521</td>\n",
       "      <td>493</td>\n",
       "      <td>481</td>\n",
       "      <td>485</td>\n",
       "      <td>490</td>\n",
       "      <td>478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>496</td>\n",
       "      <td>524</td>\n",
       "      <td>490</td>\n",
       "      <td>485</td>\n",
       "      <td>438</td>\n",
       "      <td>488</td>\n",
       "      <td>503</td>\n",
       "      <td>476</td>\n",
       "      <td>474</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>474</td>\n",
       "      <td>519</td>\n",
       "      <td>441</td>\n",
       "      <td>453</td>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "      <td>503</td>\n",
       "      <td>543</td>\n",
       "      <td>547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>486</td>\n",
       "      <td>465</td>\n",
       "      <td>481</td>\n",
       "      <td>467</td>\n",
       "      <td>529</td>\n",
       "      <td>484</td>\n",
       "      <td>464</td>\n",
       "      <td>476</td>\n",
       "      <td>508</td>\n",
       "      <td>474</td>\n",
       "      <td>...</td>\n",
       "      <td>482</td>\n",
       "      <td>454</td>\n",
       "      <td>712</td>\n",
       "      <td>425</td>\n",
       "      <td>518</td>\n",
       "      <td>479</td>\n",
       "      <td>466</td>\n",
       "      <td>494</td>\n",
       "      <td>470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479</td>\n",
       "      <td>485</td>\n",
       "      <td>502</td>\n",
       "      <td>489</td>\n",
       "      <td>487</td>\n",
       "      <td>478</td>\n",
       "      <td>402</td>\n",
       "      <td>477</td>\n",
       "      <td>500</td>\n",
       "      <td>473</td>\n",
       "      <td>...</td>\n",
       "      <td>470</td>\n",
       "      <td>491</td>\n",
       "      <td>381</td>\n",
       "      <td>532</td>\n",
       "      <td>469</td>\n",
       "      <td>488</td>\n",
       "      <td>487</td>\n",
       "      <td>539</td>\n",
       "      <td>546</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>482</td>\n",
       "      <td>485</td>\n",
       "      <td>551</td>\n",
       "      <td>475</td>\n",
       "      <td>443</td>\n",
       "      <td>475</td>\n",
       "      <td>456</td>\n",
       "      <td>475</td>\n",
       "      <td>494</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>484</td>\n",
       "      <td>479</td>\n",
       "      <td>574</td>\n",
       "      <td>509</td>\n",
       "      <td>509</td>\n",
       "      <td>473</td>\n",
       "      <td>483</td>\n",
       "      <td>545</td>\n",
       "      <td>490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0     486     497     494     477     582     478     535     477     496   \n",
       "1     496     524     490     485     438     488     503     476     474   \n",
       "2     486     465     481     467     529     484     464     476     508   \n",
       "3     479     485     502     489     487     478     402     477     500   \n",
       "4     482     485     551     475     443     475     456     475     494   \n",
       "\n",
       "   feat_9  ...  feat_491  feat_492  feat_493  feat_494  feat_495  feat_496  \\\n",
       "0     480  ...       485       473       576       521       493       481   \n",
       "1     491  ...       474       519       441       453       488       488   \n",
       "2     474  ...       482       454       712       425       518       479   \n",
       "3     473  ...       470       491       381       532       469       488   \n",
       "4     471  ...       484       479       574       509       509       473   \n",
       "\n",
       "   feat_497  feat_498  feat_499  labels  \n",
       "0       485       490       478       0  \n",
       "1       503       543       547       0  \n",
       "2       466       494       470       1  \n",
       "3       487       539       546       1  \n",
       "4       483       545       490       1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=data.drop(columns='labels',axis=0).values\n",
    "Y_train=data['labels'].values\n",
    "X_test=test_data.drop(columns='labels',axis=0).values\n",
    "Y_test=test_data['labels'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8oap0K91eKJI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07039380413419974, 0.06730895822974696, 0.05650083861124846, 0.04907269205067756, 0.047104878201523474, 0.03861436400785164, 0.03702280674369065, 0.03395065398813658, 0.03370831159367466, 0.024426644482209173, 0.0214994930381487, 0.01679743471155175, 0.016318266732065576, 0.011334585772759611, 0.008468112950880673, 0.007189925326954614, 0.0071186712143131705, 0.0069139897988606735, 0.005470490580574121, 0.004928338178097111, 0.004298382054526271, 0.004139918554829167, 0.004125882016126584, 0.003694489515254736, 0.0036937776660434917, 0.0036831751880854417, 0.0035561732963597815, 0.0034387837746616616, 0.00325568891431268, 0.002994054000224335, 0.00296909018590235, 0.002959273822540227, 0.0029207693378746047, 0.0028837276841777292, 0.002788964730238649, 0.002688220684103105, 0.0026338042559062944, 0.002630968472697144, 0.002630126797455511, 0.002622470199122718, 0.0025882842936308273, 0.0025831945805525635, 0.002575254661032398, 0.002561573116056535, 0.002535916723675385, 0.0025303813180590617, 0.0025244698911506746, 0.0025175155624798887, 0.0024986582480660785, 0.00247343616761808, 0.0024619021600498603, 0.0024340140865788413, 0.0024225694049007816, 0.002411256354152361, 0.0023948636123204393, 0.0023362196483849786, 0.002290519852512, 0.0022856302162813187, 0.0022533407104932093, 0.002242135345835345, 0.002230398836771875, 0.002220892573634582, 0.0021991658325214197, 0.0021800110541455617, 0.002173910707505513, 0.0021702146442833573, 0.0021457950792753127, 0.002139542339543014, 0.0021350914776574166, 0.002130268710289583, 0.0021146209624392713, 0.002104875948476452, 0.0020918537781197464, 0.0020892613602314356, 0.002072439334145236, 0.002057056509961847, 0.0020560395771915168, 0.002026314355852044, 0.002019550672545412, 0.0020134001105397573, 0.0019899850074850743, 0.00198848658729139, 0.001987340926400466, 0.001977849303400617, 0.0019743690009341314, 0.001965989432093029, 0.001955544797929498, 0.0019550910934212334, 0.001947134599787235, 0.0019117314030688349, 0.0019060889925493153, 0.0018945388847122805, 0.0018893884577350388, 0.0018829327111694184, 0.00186457516171262, 0.0018618224054787111, 0.0018568547226827678, 0.001812502387815431, 0.0017717642854814862, 0.0017522993425146116, 0.001736214378411308, 0.0017346368363720408, 0.0017234043481363117, 0.0017149043634302268, 0.001706020649025889, 0.0017008708449960763, 0.0016969305165948352, 0.0016915418002980342, 0.001678005670895578, 0.0016773244260227002, 0.0016696763414319477, 0.0016640920915717689, 0.0016578771828053786, 0.0016577239658187265, 0.0016306309677255318, 0.001611303477066548, 0.0016111215589008734, 0.0015967260523172204, 0.0015818711087995873, 0.0015814950268749775, 0.0015738321705589175, 0.0015731900750525266, 0.0015678024160613367, 0.0015571417454507024, 0.0015500870558234714, 0.0015493820711103613, 0.001541544202720897, 0.001499052819370454, 0.0014989558986478713, 0.001490526943632873, 0.0014862242537036793, 0.0014827587849268933, 0.0014811365697281807, 0.0014809968429354875, 0.0014728250694772914, 0.0014716653192251956, 0.0014636210608555504, 0.0014548769651897848, 0.0014497460122863905, 0.0014482929568955152, 0.0014399711207594386, 0.0014292174447503468, 0.0014214582147767528, 0.0014093851538713306, 0.0013971743792164108, 0.0013942350307111972, 0.0013826580404112593, 0.001366568830664876, 0.0013645698590289738, 0.001353721775755248, 0.0013432652981725131, 0.0013422653468015088, 0.0013322622085991324, 0.001331080512807181, 0.0013286232938203542, 0.0013267205044290946, 0.0013186509243350824, 0.001313599887613612, 0.0013054521958597092, 0.0013017291863983763, 0.0012795102491501996, 0.0012711589557251476, 0.0012492113024776832, 0.0012471103686316736, 0.0012444438502900996, 0.001244349424028586, 0.0012409450973710967, 0.00123640248039034, 0.0012272172679863586, 0.0012194644446400138, 0.0012160233608432302, 0.0012151279065383558, 0.0012104399050624148, 0.0011952166122777236, 0.0011799700089554334, 0.0011730554161261022, 0.001151671436005947, 0.0011495607815060003, 0.0011340494857525563, 0.001130702747859856, 0.0011284427465270697, 0.0011277113293450436, 0.0011276311688274217, 0.0011104361677654656, 0.0011040172666071473, 0.0011011457233793987, 0.001089599524733874, 0.0010760347836173743, 0.0010725899102884547, 0.0010718427462919897, 0.001068132574505767, 0.0010673750335577082, 0.0010657047263383128, 0.0010646307825896953, 0.0010541837765236021, 0.0010489333658013431, 0.0010470645870720914, 0.0010469936822923866, 0.0010402823997841893, 0.0010288563492453632, 0.0010203235447027758, 0.0010169783936549214, 0.001006977363749057, 0.0010013287399090365, 0.000998095056819495, 0.0009915771020785145, 0.000989082781230348, 0.000987863233909045, 0.000986682704532976, 0.0009860776653391856, 0.0009681801954253284, 0.000956820101114027, 0.0009563007181827107, 0.000945230735683899, 0.0009446725529352384, 0.0009385514765058968, 0.0009339980385769422, 0.0009314644486714325, 0.0009275583537350098, 0.0009215323876388936, 0.0009189473102237635, 0.0009172550303045822, 0.0009112559767529433, 0.0009058976181207855, 0.0009000428473445559, 0.0008989970127672481, 0.0008926531620693641, 0.0008874366098570937, 0.0008805732067352111, 0.0008798314195002234, 0.0008787175312669373, 0.0008677093764186328, 0.0008670250332947393, 0.0008664306587796743, 0.0008627502268263698, 0.0008593929108625407, 0.0008524160343878771, 0.0008510955434678474, 0.0008479650941107646, 0.0008409662595378404, 0.0008407112350074732, 0.0008252964575983673, 0.0008207361737170579, 0.000794877740281447, 0.0007808338650646563, 0.0007717409311351965, 0.000768350980111556, 0.0007643889912270703, 0.0007636762635321925, 0.0007636332971822749, 0.0007592339671764409, 0.0007585038711340793, 0.0007555860425291176, 0.0007501290490651372, 0.0007493583877783136, 0.0007389271054335182, 0.0007232430985496511, 0.0007175723209437687, 0.0007074913313071446, 0.0006954176657943589, 0.0006847711713340302, 0.0006791916780548589, 0.0006774676621126858, 0.0006636982497144816, 0.0006632453869914374, 0.0006594902484329937, 0.0006590790007888857, 0.0006569019374631696, 0.0006504947420295396, 0.0006456465542947096, 0.0006436487295954904, 0.0006334784151789082, 0.0006307057349429491, 0.0006241440055432718, 0.0006218702345059491, 0.0006164234217537295, 0.0006116984777074654, 0.0006103906154957196, 0.0006004998368407976, 0.0006004315815953162, 0.0006002679961323757, 0.0005990686474946357, 0.0005973443356215204, 0.0005971172501921796, 0.0005944886244654918, 0.0005880506560300827, 0.0005744034773723301, 0.0005686881694526522, 0.0005584191029353392, 0.0005579404115994966, 0.000557730392563297, 0.0005575542570685673, 0.0005559948043511279, 0.00055394353169171, 0.0005506636755894485, 0.0005449461361805205, 0.0005427302851766466, 0.0005408163881614291, 0.0005296178647347794, 0.0005197456197915037, 0.0005056150819460992, 0.0004951776632391799, 0.000494172350277419, 0.00047765566215032237, 0.0004683277064584979, 0.00045544631990928085, 0.0004465700035184638, 0.0004342173164003043, 0.00043279617750912523, 0.00043253933513674844, 0.0004325325701222137, 0.0004262182834686826, 0.00042376933631211515, 0.0004124240551100672, 0.0004070191169258485, 0.00040691217398257075, 0.00040340736429949486, 0.00039717185997393585, 0.0003963604131757799, 0.000394025635503021, 0.0003936036243675585, 0.00039036487806597164, 0.00039021630829181144, 0.00038992582756166053, 0.0003875985975104429, 0.0003870088611247273, 0.0003854340814286214, 0.0003852273068079966, 0.0003847094285307292, 0.0003813164436695527, 0.0003800002946166935, 0.00037876252164657694, 0.0003737596768578452, 0.0003713474824200901, 0.0003702417795671112, 0.00036571859587569325, 0.0003581440163784869, 0.0003556655600962463, 0.00035450051920163107, 0.0003527497872237695, 0.000350876431335936, 0.00033492750263884797, 0.00033492750263884786, 0.00031704593846760247, 0.00031526489031956194, 0.0002716212383161647, 0.0002701173081049198, 0.0002634884208533099, 0.00026331021083586737, 0.0002627018410746794, 0.00025894094112103894, 0.0002573352355053042, 0.00025574327495772045, 0.00024783538681428293, 0.000214462567500827, 0.00020882235380673762, 0.00020457852249429365, 0.0002030294877429064, 0.0002029722689603008, 0.00020270464056251777, 0.00020263015561339568, 0.00020224451230721555, 0.00020214409815999957, 0.00020111087147015613, 0.00020097951878006876, 0.00020096707047646925, 0.00020050559646115766, 0.0002002389826684677, 0.0001998658810492014, 0.00019963832700088736, 0.00019946288351932212, 0.0001994131617411659, 0.00019926443279604365, 0.00019780452901930524, 0.0001969091729649578, 0.00019633874909628801, 0.00019483871404289308, 0.000194546514048851, 0.00019390539626459616, 0.00019138391850467468, 0.0001893812608232885, 0.0001889334630270422, 0.00018598240885912506, 0.00018406757727776996, 0.0001789545890200542, 0.00017731157155580153, 0.00017054476970404132, 0.00017054476970404132, 0.00017043294192386113, 0.00017043294192386113, 0.00016814127483709957, 0.00016374233462343669, 0.0001614655218312654, 0.0001594134933078215, 0.00015198134704782988, 0.0001513739267168113, 0.00014170088294028577, 0.00014170088294028577, 0.00014028241542465817, 0.0001364358157632331, 0.00013488558307386993, 0.00013455460152605447, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['feat_493', 'feat_28', 'feat_105', 'feat_318', 'feat_153', 'feat_48', 'feat_442', 'feat_475', 'feat_378', 'feat_338']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(random_state=42, max_depth=10)\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "features=list(data.columns)\n",
    "features.remove('labels')\n",
    "importances = model.feature_importances_\n",
    "print(sorted(importances,reverse=True))\n",
    "indices = np.argsort(importances)[-10:]  # top 10 features\n",
    "feature_selection=[features[i] for i in indices]\n",
    "print(feature_selection)\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I have used the feature importance attribute of Random Forest algorithm in sklearn to filter out the important features from the data-set. In the graph above, I have plotted the relative importance of the top-ten important attributes (as identified by the algorithm) in the data-set. I have used these top ten features (['feat_493', 'feat_28', 'feat_105', 'feat_318', 'feat_153', 'feat_48', 'feat_442', 'feat_475', 'feat_378', 'feat_338']) to build the models below. As, one can observe, all the models performed better after feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection=['feat_493', 'feat_28', 'feat_105', 'feat_318', 'feat_153', 'feat_48', 'feat_442', 'feat_475', 'feat_378', 'feat_338']\n",
    "\n",
    "X_train=data.drop(columns='labels',axis=0)[feature_selection].values\n",
    "Y_train=data['labels'].values\n",
    "X_test=test_data.drop(columns='labels',axis=0)[feature_selection].values\n",
    "Y_test=test_data['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6045\n",
      "0.5983333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2543035d9b0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_scores=[]\n",
    "test_scores=[]\n",
    "r_lambda=[0.1,0.5,1.5,3,3.5,5,8,10,15,20,50,100,1000]\n",
    "for i in r_lambda:\n",
    "  classifier=LogisticRegression(penalty='l2',C=1/i)\n",
    "  classifier.fit(X_train,Y_train)\n",
    "  training_prediction=classifier.predict(X_train)\n",
    "  test_prediction=classifier.predict(X_test)\n",
    "  training_scores.append(accuracy_score(training_prediction,Y_train))\n",
    "  test_scores.append(accuracy_score(test_prediction,Y_test))\n",
    "\n",
    "print(min(training_scores))\n",
    "print(max(test_scores))\n",
    "\n",
    "plt.plot(r_lambda,training_scores,\"b\",label=\"training accuracy\")\n",
    "plt.plot(r_lambda,test_scores,\"r\",label=\"test accuracy\")\n",
    "plt.xlabel(\"Lambda λ\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQDjmxE1f85p"
   },
   "source": [
    "Task 4: Model building & evaluation. Train at least 4 models:\n",
    "\n",
    "\n",
    "1.   Neural network (any type of NN is fine)\n",
    "2.   Decision tree (can be a plain decision tree, random forest, gradient boosted trees, etc.)\n",
    "3.   Support vector machine\n",
    "4.   Your choice of Naive Bayes or K-nearest neighbors\n",
    "\n",
    "For model 4, briefly (no more than 2 paragraphs) describe how the model works. \n",
    "\n",
    "Part of your grade will depend on how your best model performs against the best model of all the classmates, as determined by the accuracy achieved using the test set. You are encouraged to share your best model's accuracy on Piazza, but not which feature engineering or model tuning steps you took to achieve it. (60 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQ9ilU7Hf_GT"
   },
   "outputs": [],
   "source": [
    "#Neural Network Implementation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt  \n",
    "import seaborn as sns\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>423</td>\n",
       "      <td>504</td>\n",
       "      <td>493</td>\n",
       "      <td>521</td>\n",
       "      <td>466</td>\n",
       "      <td>494</td>\n",
       "      <td>479</td>\n",
       "      <td>482</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>478</td>\n",
       "      <td>479</td>\n",
       "      <td>567</td>\n",
       "      <td>547</td>\n",
       "      <td>498</td>\n",
       "      <td>484</td>\n",
       "      <td>474</td>\n",
       "      <td>567</td>\n",
       "      <td>538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483</td>\n",
       "      <td>499</td>\n",
       "      <td>520</td>\n",
       "      <td>467</td>\n",
       "      <td>495</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>477</td>\n",
       "      <td>488</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>481</td>\n",
       "      <td>484</td>\n",
       "      <td>451</td>\n",
       "      <td>445</td>\n",
       "      <td>443</td>\n",
       "      <td>481</td>\n",
       "      <td>485</td>\n",
       "      <td>492</td>\n",
       "      <td>477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>487</td>\n",
       "      <td>486</td>\n",
       "      <td>495</td>\n",
       "      <td>481</td>\n",
       "      <td>421</td>\n",
       "      <td>481</td>\n",
       "      <td>499</td>\n",
       "      <td>478</td>\n",
       "      <td>489</td>\n",
       "      <td>482</td>\n",
       "      <td>...</td>\n",
       "      <td>477</td>\n",
       "      <td>511</td>\n",
       "      <td>245</td>\n",
       "      <td>522</td>\n",
       "      <td>480</td>\n",
       "      <td>483</td>\n",
       "      <td>493</td>\n",
       "      <td>421</td>\n",
       "      <td>488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480</td>\n",
       "      <td>427</td>\n",
       "      <td>531</td>\n",
       "      <td>458</td>\n",
       "      <td>544</td>\n",
       "      <td>492</td>\n",
       "      <td>489</td>\n",
       "      <td>477</td>\n",
       "      <td>478</td>\n",
       "      <td>482</td>\n",
       "      <td>...</td>\n",
       "      <td>483</td>\n",
       "      <td>471</td>\n",
       "      <td>313</td>\n",
       "      <td>490</td>\n",
       "      <td>414</td>\n",
       "      <td>480</td>\n",
       "      <td>516</td>\n",
       "      <td>495</td>\n",
       "      <td>469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>491</td>\n",
       "      <td>472</td>\n",
       "      <td>430</td>\n",
       "      <td>463</td>\n",
       "      <td>431</td>\n",
       "      <td>480</td>\n",
       "      <td>459</td>\n",
       "      <td>477</td>\n",
       "      <td>481</td>\n",
       "      <td>479</td>\n",
       "      <td>...</td>\n",
       "      <td>479</td>\n",
       "      <td>493</td>\n",
       "      <td>435</td>\n",
       "      <td>444</td>\n",
       "      <td>455</td>\n",
       "      <td>482</td>\n",
       "      <td>468</td>\n",
       "      <td>497</td>\n",
       "      <td>435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0     485     423     504     493     521     466     494     479     482   \n",
       "1     483     499     520     467     495     484     485     477     488   \n",
       "2     487     486     495     481     421     481     499     478     489   \n",
       "3     480     427     531     458     544     492     489     477     478   \n",
       "4     491     472     430     463     431     480     459     477     481   \n",
       "\n",
       "   feat_9  ...  feat_491  feat_492  feat_493  feat_494  feat_495  feat_496  \\\n",
       "0     471  ...       478       479       567       547       498       484   \n",
       "1     491  ...       481       484       451       445       443       481   \n",
       "2     482  ...       477       511       245       522       480       483   \n",
       "3     482  ...       483       471       313       490       414       480   \n",
       "4     479  ...       479       493       435       444       455       482   \n",
       "\n",
       "   feat_497  feat_498  feat_499  labels  \n",
       "0       474       567       538       0  \n",
       "1       485       492       477       1  \n",
       "2       493       421       488       1  \n",
       "3       516       495       469       0  \n",
       "4       468       497       435       1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('a4-train.csv', index_col=0)\n",
    "test_data=pd.read_csv('a4-test.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>486</td>\n",
       "      <td>497</td>\n",
       "      <td>494</td>\n",
       "      <td>477</td>\n",
       "      <td>582</td>\n",
       "      <td>478</td>\n",
       "      <td>535</td>\n",
       "      <td>477</td>\n",
       "      <td>496</td>\n",
       "      <td>480</td>\n",
       "      <td>...</td>\n",
       "      <td>485</td>\n",
       "      <td>473</td>\n",
       "      <td>576</td>\n",
       "      <td>521</td>\n",
       "      <td>493</td>\n",
       "      <td>481</td>\n",
       "      <td>485</td>\n",
       "      <td>490</td>\n",
       "      <td>478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>496</td>\n",
       "      <td>524</td>\n",
       "      <td>490</td>\n",
       "      <td>485</td>\n",
       "      <td>438</td>\n",
       "      <td>488</td>\n",
       "      <td>503</td>\n",
       "      <td>476</td>\n",
       "      <td>474</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>474</td>\n",
       "      <td>519</td>\n",
       "      <td>441</td>\n",
       "      <td>453</td>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "      <td>503</td>\n",
       "      <td>543</td>\n",
       "      <td>547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>486</td>\n",
       "      <td>465</td>\n",
       "      <td>481</td>\n",
       "      <td>467</td>\n",
       "      <td>529</td>\n",
       "      <td>484</td>\n",
       "      <td>464</td>\n",
       "      <td>476</td>\n",
       "      <td>508</td>\n",
       "      <td>474</td>\n",
       "      <td>...</td>\n",
       "      <td>482</td>\n",
       "      <td>454</td>\n",
       "      <td>712</td>\n",
       "      <td>425</td>\n",
       "      <td>518</td>\n",
       "      <td>479</td>\n",
       "      <td>466</td>\n",
       "      <td>494</td>\n",
       "      <td>470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479</td>\n",
       "      <td>485</td>\n",
       "      <td>502</td>\n",
       "      <td>489</td>\n",
       "      <td>487</td>\n",
       "      <td>478</td>\n",
       "      <td>402</td>\n",
       "      <td>477</td>\n",
       "      <td>500</td>\n",
       "      <td>473</td>\n",
       "      <td>...</td>\n",
       "      <td>470</td>\n",
       "      <td>491</td>\n",
       "      <td>381</td>\n",
       "      <td>532</td>\n",
       "      <td>469</td>\n",
       "      <td>488</td>\n",
       "      <td>487</td>\n",
       "      <td>539</td>\n",
       "      <td>546</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>482</td>\n",
       "      <td>485</td>\n",
       "      <td>551</td>\n",
       "      <td>475</td>\n",
       "      <td>443</td>\n",
       "      <td>475</td>\n",
       "      <td>456</td>\n",
       "      <td>475</td>\n",
       "      <td>494</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>484</td>\n",
       "      <td>479</td>\n",
       "      <td>574</td>\n",
       "      <td>509</td>\n",
       "      <td>509</td>\n",
       "      <td>473</td>\n",
       "      <td>483</td>\n",
       "      <td>545</td>\n",
       "      <td>490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0     486     497     494     477     582     478     535     477     496   \n",
       "1     496     524     490     485     438     488     503     476     474   \n",
       "2     486     465     481     467     529     484     464     476     508   \n",
       "3     479     485     502     489     487     478     402     477     500   \n",
       "4     482     485     551     475     443     475     456     475     494   \n",
       "\n",
       "   feat_9  ...  feat_491  feat_492  feat_493  feat_494  feat_495  feat_496  \\\n",
       "0     480  ...       485       473       576       521       493       481   \n",
       "1     491  ...       474       519       441       453       488       488   \n",
       "2     474  ...       482       454       712       425       518       479   \n",
       "3     473  ...       470       491       381       532       469       488   \n",
       "4     471  ...       484       479       574       509       509       473   \n",
       "\n",
       "   feat_497  feat_498  feat_499  labels  \n",
       "0       485       490       478       0  \n",
       "1       503       543       547       0  \n",
       "2       466       494       470       1  \n",
       "3       487       539       546       1  \n",
       "4       483       545       490       1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=data.drop(columns='labels',axis=0).values\n",
    "Y_train=data['labels'].values\n",
    "X_test=test_data.drop(columns='labels',axis=0).values\n",
    "Y_test=test_data['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following is a baseline model without any feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(125, input_dim=500, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 125)               62625     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                4032      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 66,690\n",
      "Trainable params: 66,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2000/2000 [==============================] - 0s 178us/step - loss: 0.7218 - accuracy: 0.5325\n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 0.4719 - accuracy: 0.8320\n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - 0s 68us/step - loss: 0.3154 - accuracy: 0.9290\n",
      "Epoch 4/100\n",
      "2000/2000 [==============================] - 0s 66us/step - loss: 0.1813 - accuracy: 0.9775\n",
      "Epoch 5/100\n",
      "2000/2000 [==============================] - 0s 78us/step - loss: 0.0909 - accuracy: 0.9955\n",
      "Epoch 6/100\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 0.0440 - accuracy: 0.9995\n",
      "Epoch 7/100\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "2000/2000 [==============================] - 0s 84us/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "2000/2000 [==============================] - 0s 76us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "2000/2000 [==============================] - 0s 80us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 9.0556e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 7.6209e-04 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 6.4755e-04 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 5.5373e-04 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 4.7626e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "2000/2000 [==============================] - 0s 70us/step - loss: 4.1226e-04 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "2000/2000 [==============================] - 0s 84us/step - loss: 3.5904e-04 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 3.1413e-04 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 2.7630e-04 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 2.4425e-04 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 2.1664e-04 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "2000/2000 [==============================] - 0s 78us/step - loss: 1.9278e-04 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 1.7249e-04 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 1.5482e-04 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 1.3938e-04 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "2000/2000 [==============================] - 0s 78us/step - loss: 1.2606e-04 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "2000/2000 [==============================] - 0s 70us/step - loss: 1.1430e-04 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "2000/2000 [==============================] - 0s 67us/step - loss: 1.0388e-04 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 9.4638e-05 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 8.6615e-05 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "2000/2000 [==============================] - 0s 78us/step - loss: 7.9222e-05 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 7.2754e-05 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "2000/2000 [==============================] - 0s 72us/step - loss: 6.6982e-05 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 6.1719e-05 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 5.7038e-05 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 5.2796e-05 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "2000/2000 [==============================] - 0s 76us/step - loss: 4.8934e-05 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "2000/2000 [==============================] - 0s 66us/step - loss: 4.5468e-05 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "2000/2000 [==============================] - 0s 68us/step - loss: 4.2249e-05 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "2000/2000 [==============================] - 0s 67us/step - loss: 3.9344e-05 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "2000/2000 [==============================] - 0s 68us/step - loss: 3.6689e-05 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "2000/2000 [==============================] - 0s 71us/step - loss: 3.4265e-05 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 3.2024e-05 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "2000/2000 [==============================] - 0s 80us/step - loss: 2.9960e-05 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "2000/2000 [==============================] - 0s 72us/step - loss: 2.8074e-05 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 2.6342e-05 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "2000/2000 [==============================] - 0s 78us/step - loss: 2.4732e-05 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 2.3238e-05 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 2.1859e-05 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 2.0595e-05 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "2000/2000 [==============================] - 0s 62us/step - loss: 1.9392e-05 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "2000/2000 [==============================] - 0s 66us/step - loss: 1.8291e-05 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 1.7259e-05 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "2000/2000 [==============================] - 0s 65us/step - loss: 1.6299e-05 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "2000/2000 [==============================] - 0s 62us/step - loss: 1.5404e-05 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 1.4567e-05 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 1.3786e-05 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 1.3056e-05 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 1.2370e-05 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 1.1729e-05 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 1.1126e-05 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 1.0556e-05 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 1.0034e-05 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 9.5226e-06 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 9.0520e-06 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 8.6092e-06 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "2000/2000 [==============================] - 0s 62us/step - loss: 8.1886e-06 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "2000/2000 [==============================] - 0s 65us/step - loss: 7.7892e-06 - accuracy: 1.0000\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 68us/step - loss: 7.4171e-06 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "2000/2000 [==============================] - 0s 62us/step - loss: 7.0621e-06 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 6.7321e-06 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 6.4148e-06 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 6.1160e-06 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "2000/2000 [==============================] - 0s 62us/step - loss: 5.8354e-06 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "2000/2000 [==============================] - 0s 66us/step - loss: 5.5679e-06 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "2000/2000 [==============================] - 0s 68us/step - loss: 5.3133e-06 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "2000/2000 [==============================] - 0s 70us/step - loss: 5.0715e-06 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 4.8442e-06 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 4.6273e-06 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 4.4205e-06 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 4.2241e-06 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 4.0398e-06 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 3.8608e-06 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 3.6935e-06 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 3.5345e-06 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 3.3831e-06 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 3.2373e-06 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "2000/2000 [==============================] - 0s 66us/step - loss: 3.1010e-06 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 2.9690e-06 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 2.8440e-06 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "2000/2000 [==============================] - 0s 65us/step - loss: 2.7244e-06 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 2.6105e-06 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 2.5021e-06 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_train_pred=model.predict(X_train)\n",
    "#print(y_pred)\n",
    "for i in range(0,len(y_pred)):\n",
    "  if y_pred[i]>=0.5:\n",
    "    y_pred[i]=1\n",
    "  else:\n",
    "    y_pred[i]=0\n",
    "\n",
    "for i in range(0,len(y_train_pred)):\n",
    "  if y_train_pred[i]>=0.5:\n",
    "    y_train_pred[i]=1\n",
    "  else:\n",
    "    y_train_pred[i]=0\n",
    "\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 1.0\n",
      "Test accuracy 0.565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Training accuracy',accuracy_score(y_train_pred,Y_train))\n",
    "print('Test accuracy',accuracy_score(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection=['feat_493', 'feat_28', 'feat_105', 'feat_318', 'feat_153', 'feat_48', 'feat_442', 'feat_475', 'feat_378', 'feat_338']\n",
    "\n",
    "\n",
    "X_train=data.drop(columns='labels',axis=0)[feature_selection].values\n",
    "Y_train=data['labels'].values\n",
    "X_test=test_data.drop(columns='labels',axis=0)[feature_selection].values\n",
    "Y_test=test_data['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(20, input_dim=10, activation='relu'))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 105       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 331\n",
      "Trainable params: 331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.7274 - accuracy: 0.4880\n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.6238 - accuracy: 0.6980\n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5892 - accuracy: 0.7085\n",
      "Epoch 4/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.5668 - accuracy: 0.7315\n",
      "Epoch 5/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.5479 - accuracy: 0.7560\n",
      "Epoch 6/100\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5310 - accuracy: 0.7690\n",
      "Epoch 7/100\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.5150 - accuracy: 0.7780\n",
      "Epoch 8/100\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.4996 - accuracy: 0.7895\n",
      "Epoch 9/100\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.4866 - accuracy: 0.7890\n",
      "Epoch 10/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.4742 - accuracy: 0.7925\n",
      "Epoch 11/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.4641 - accuracy: 0.8030\n",
      "Epoch 12/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.4546 - accuracy: 0.8020\n",
      "Epoch 13/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.4467 - accuracy: 0.8065\n",
      "Epoch 14/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.4393 - accuracy: 0.8105\n",
      "Epoch 15/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.4318 - accuracy: 0.8175\n",
      "Epoch 16/100\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.4256 - accuracy: 0.8185\n",
      "Epoch 17/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.4189 - accuracy: 0.8275\n",
      "Epoch 18/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.4146 - accuracy: 0.8305\n",
      "Epoch 19/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.4094 - accuracy: 0.8275\n",
      "Epoch 20/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.4028 - accuracy: 0.8300\n",
      "Epoch 21/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3979 - accuracy: 0.8355\n",
      "Epoch 22/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.3929 - accuracy: 0.8345\n",
      "Epoch 23/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3880 - accuracy: 0.8390\n",
      "Epoch 24/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3844 - accuracy: 0.8395\n",
      "Epoch 25/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.3799 - accuracy: 0.8420\n",
      "Epoch 26/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.3757 - accuracy: 0.8500\n",
      "Epoch 27/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.3707 - accuracy: 0.8460\n",
      "Epoch 28/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3665 - accuracy: 0.8475\n",
      "Epoch 29/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.3639 - accuracy: 0.8490\n",
      "Epoch 30/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3590 - accuracy: 0.8540\n",
      "Epoch 31/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.3555 - accuracy: 0.8540\n",
      "Epoch 32/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3517 - accuracy: 0.8575\n",
      "Epoch 33/100\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.3486 - accuracy: 0.8585\n",
      "Epoch 34/100\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.3459 - accuracy: 0.8605\n",
      "Epoch 35/100\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.3422 - accuracy: 0.8595\n",
      "Epoch 36/100\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.3393 - accuracy: 0.8630\n",
      "Epoch 37/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3390 - accuracy: 0.8600\n",
      "Epoch 38/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3328 - accuracy: 0.8650\n",
      "Epoch 39/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3317 - accuracy: 0.8615\n",
      "Epoch 40/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3287 - accuracy: 0.8650\n",
      "Epoch 41/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3273 - accuracy: 0.8675\n",
      "Epoch 42/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3248 - accuracy: 0.8650\n",
      "Epoch 43/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3244 - accuracy: 0.8680\n",
      "Epoch 44/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.3221 - accuracy: 0.8680\n",
      "Epoch 45/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.3186 - accuracy: 0.8705\n",
      "Epoch 46/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3174 - accuracy: 0.8685\n",
      "Epoch 47/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.3156 - accuracy: 0.8690\n",
      "Epoch 48/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3142 - accuracy: 0.8695\n",
      "Epoch 49/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.3113 - accuracy: 0.8715\n",
      "Epoch 50/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.3114 - accuracy: 0.8720\n",
      "Epoch 51/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3109 - accuracy: 0.8710\n",
      "Epoch 52/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.3085 - accuracy: 0.8730\n",
      "Epoch 53/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.3064 - accuracy: 0.8725\n",
      "Epoch 54/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3050 - accuracy: 0.8720\n",
      "Epoch 55/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.3046 - accuracy: 0.8725\n",
      "Epoch 56/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.3019 - accuracy: 0.8790\n",
      "Epoch 57/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.3018 - accuracy: 0.8720\n",
      "Epoch 58/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.3009 - accuracy: 0.8775\n",
      "Epoch 59/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.3008 - accuracy: 0.8790\n",
      "Epoch 60/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2996 - accuracy: 0.8780\n",
      "Epoch 61/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2983 - accuracy: 0.8750\n",
      "Epoch 62/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2968 - accuracy: 0.8745\n",
      "Epoch 63/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2957 - accuracy: 0.8755\n",
      "Epoch 64/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2940 - accuracy: 0.8765\n",
      "Epoch 65/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2931 - accuracy: 0.8795\n",
      "Epoch 66/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2925 - accuracy: 0.8755\n",
      "Epoch 67/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2911 - accuracy: 0.8755\n",
      "Epoch 68/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2904 - accuracy: 0.8775\n",
      "Epoch 69/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2903 - accuracy: 0.8770\n",
      "Epoch 70/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2892 - accuracy: 0.8775\n",
      "Epoch 71/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2886 - accuracy: 0.8775\n",
      "Epoch 72/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2879 - accuracy: 0.8805\n",
      "Epoch 73/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2864 - accuracy: 0.8835\n",
      "Epoch 74/100\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2862 - accuracy: 0.8805\n",
      "Epoch 75/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2865 - accuracy: 0.8825\n",
      "Epoch 76/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2861 - accuracy: 0.8785\n",
      "Epoch 77/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2853 - accuracy: 0.8805\n",
      "Epoch 78/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2831 - accuracy: 0.8835\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2837 - accuracy: 0.8820\n",
      "Epoch 80/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2840 - accuracy: 0.8820\n",
      "Epoch 81/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2815 - accuracy: 0.8830\n",
      "Epoch 82/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2818 - accuracy: 0.8800\n",
      "Epoch 83/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2795 - accuracy: 0.8810\n",
      "Epoch 84/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2803 - accuracy: 0.8830\n",
      "Epoch 85/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2805 - accuracy: 0.8870\n",
      "Epoch 86/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2790 - accuracy: 0.8855\n",
      "Epoch 87/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2783 - accuracy: 0.8850\n",
      "Epoch 88/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2807 - accuracy: 0.8825\n",
      "Epoch 89/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2776 - accuracy: 0.8850\n",
      "Epoch 90/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2762 - accuracy: 0.8850\n",
      "Epoch 91/100\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2772 - accuracy: 0.8860\n",
      "Epoch 92/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2755 - accuracy: 0.8870\n",
      "Epoch 93/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2757 - accuracy: 0.8875\n",
      "Epoch 94/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2739 - accuracy: 0.8840\n",
      "Epoch 95/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2736 - accuracy: 0.8865\n",
      "Epoch 96/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2727 - accuracy: 0.8845\n",
      "Epoch 97/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2741 - accuracy: 0.8845\n",
      "Epoch 98/100\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2737 - accuracy: 0.8910\n",
      "Epoch 99/100\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2750 - accuracy: 0.8865\n",
      "Epoch 100/100\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2710 - accuracy: 0.8865\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_train_pred=model.predict(X_train)\n",
    "#print(y_pred)\n",
    "for i in range(0,len(y_pred)):\n",
    "  if y_pred[i]>=0.5:\n",
    "    y_pred[i]=1\n",
    "  else:\n",
    "    y_pred[i]=0\n",
    "\n",
    "for i in range(0,len(y_train_pred)):\n",
    "  if y_train_pred[i]>=0.5:\n",
    "    y_train_pred[i]=1\n",
    "  else:\n",
    "    y_train_pred[i]=0\n",
    "\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.8945\n",
      "Test accuracy 0.8583333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Training accuracy',accuracy_score(y_train_pred,Y_train))\n",
    "print('Test accuracy',accuracy_score(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt  \n",
    "import seaborn as sns\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>423</td>\n",
       "      <td>504</td>\n",
       "      <td>493</td>\n",
       "      <td>521</td>\n",
       "      <td>466</td>\n",
       "      <td>494</td>\n",
       "      <td>479</td>\n",
       "      <td>482</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>478</td>\n",
       "      <td>479</td>\n",
       "      <td>567</td>\n",
       "      <td>547</td>\n",
       "      <td>498</td>\n",
       "      <td>484</td>\n",
       "      <td>474</td>\n",
       "      <td>567</td>\n",
       "      <td>538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483</td>\n",
       "      <td>499</td>\n",
       "      <td>520</td>\n",
       "      <td>467</td>\n",
       "      <td>495</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>477</td>\n",
       "      <td>488</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>481</td>\n",
       "      <td>484</td>\n",
       "      <td>451</td>\n",
       "      <td>445</td>\n",
       "      <td>443</td>\n",
       "      <td>481</td>\n",
       "      <td>485</td>\n",
       "      <td>492</td>\n",
       "      <td>477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>487</td>\n",
       "      <td>486</td>\n",
       "      <td>495</td>\n",
       "      <td>481</td>\n",
       "      <td>421</td>\n",
       "      <td>481</td>\n",
       "      <td>499</td>\n",
       "      <td>478</td>\n",
       "      <td>489</td>\n",
       "      <td>482</td>\n",
       "      <td>...</td>\n",
       "      <td>477</td>\n",
       "      <td>511</td>\n",
       "      <td>245</td>\n",
       "      <td>522</td>\n",
       "      <td>480</td>\n",
       "      <td>483</td>\n",
       "      <td>493</td>\n",
       "      <td>421</td>\n",
       "      <td>488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480</td>\n",
       "      <td>427</td>\n",
       "      <td>531</td>\n",
       "      <td>458</td>\n",
       "      <td>544</td>\n",
       "      <td>492</td>\n",
       "      <td>489</td>\n",
       "      <td>477</td>\n",
       "      <td>478</td>\n",
       "      <td>482</td>\n",
       "      <td>...</td>\n",
       "      <td>483</td>\n",
       "      <td>471</td>\n",
       "      <td>313</td>\n",
       "      <td>490</td>\n",
       "      <td>414</td>\n",
       "      <td>480</td>\n",
       "      <td>516</td>\n",
       "      <td>495</td>\n",
       "      <td>469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>491</td>\n",
       "      <td>472</td>\n",
       "      <td>430</td>\n",
       "      <td>463</td>\n",
       "      <td>431</td>\n",
       "      <td>480</td>\n",
       "      <td>459</td>\n",
       "      <td>477</td>\n",
       "      <td>481</td>\n",
       "      <td>479</td>\n",
       "      <td>...</td>\n",
       "      <td>479</td>\n",
       "      <td>493</td>\n",
       "      <td>435</td>\n",
       "      <td>444</td>\n",
       "      <td>455</td>\n",
       "      <td>482</td>\n",
       "      <td>468</td>\n",
       "      <td>497</td>\n",
       "      <td>435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0     485     423     504     493     521     466     494     479     482   \n",
       "1     483     499     520     467     495     484     485     477     488   \n",
       "2     487     486     495     481     421     481     499     478     489   \n",
       "3     480     427     531     458     544     492     489     477     478   \n",
       "4     491     472     430     463     431     480     459     477     481   \n",
       "\n",
       "   feat_9  ...  feat_491  feat_492  feat_493  feat_494  feat_495  feat_496  \\\n",
       "0     471  ...       478       479       567       547       498       484   \n",
       "1     491  ...       481       484       451       445       443       481   \n",
       "2     482  ...       477       511       245       522       480       483   \n",
       "3     482  ...       483       471       313       490       414       480   \n",
       "4     479  ...       479       493       435       444       455       482   \n",
       "\n",
       "   feat_497  feat_498  feat_499  labels  \n",
       "0       474       567       538       0  \n",
       "1       485       492       477       1  \n",
       "2       493       421       488       1  \n",
       "3       516       495       469       0  \n",
       "4       468       497       435       1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('a4-train.csv', index_col=0)\n",
    "test_data=pd.read_csv('a4-test.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>486</td>\n",
       "      <td>497</td>\n",
       "      <td>494</td>\n",
       "      <td>477</td>\n",
       "      <td>582</td>\n",
       "      <td>478</td>\n",
       "      <td>535</td>\n",
       "      <td>477</td>\n",
       "      <td>496</td>\n",
       "      <td>480</td>\n",
       "      <td>...</td>\n",
       "      <td>485</td>\n",
       "      <td>473</td>\n",
       "      <td>576</td>\n",
       "      <td>521</td>\n",
       "      <td>493</td>\n",
       "      <td>481</td>\n",
       "      <td>485</td>\n",
       "      <td>490</td>\n",
       "      <td>478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>496</td>\n",
       "      <td>524</td>\n",
       "      <td>490</td>\n",
       "      <td>485</td>\n",
       "      <td>438</td>\n",
       "      <td>488</td>\n",
       "      <td>503</td>\n",
       "      <td>476</td>\n",
       "      <td>474</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>474</td>\n",
       "      <td>519</td>\n",
       "      <td>441</td>\n",
       "      <td>453</td>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "      <td>503</td>\n",
       "      <td>543</td>\n",
       "      <td>547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>486</td>\n",
       "      <td>465</td>\n",
       "      <td>481</td>\n",
       "      <td>467</td>\n",
       "      <td>529</td>\n",
       "      <td>484</td>\n",
       "      <td>464</td>\n",
       "      <td>476</td>\n",
       "      <td>508</td>\n",
       "      <td>474</td>\n",
       "      <td>...</td>\n",
       "      <td>482</td>\n",
       "      <td>454</td>\n",
       "      <td>712</td>\n",
       "      <td>425</td>\n",
       "      <td>518</td>\n",
       "      <td>479</td>\n",
       "      <td>466</td>\n",
       "      <td>494</td>\n",
       "      <td>470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479</td>\n",
       "      <td>485</td>\n",
       "      <td>502</td>\n",
       "      <td>489</td>\n",
       "      <td>487</td>\n",
       "      <td>478</td>\n",
       "      <td>402</td>\n",
       "      <td>477</td>\n",
       "      <td>500</td>\n",
       "      <td>473</td>\n",
       "      <td>...</td>\n",
       "      <td>470</td>\n",
       "      <td>491</td>\n",
       "      <td>381</td>\n",
       "      <td>532</td>\n",
       "      <td>469</td>\n",
       "      <td>488</td>\n",
       "      <td>487</td>\n",
       "      <td>539</td>\n",
       "      <td>546</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>482</td>\n",
       "      <td>485</td>\n",
       "      <td>551</td>\n",
       "      <td>475</td>\n",
       "      <td>443</td>\n",
       "      <td>475</td>\n",
       "      <td>456</td>\n",
       "      <td>475</td>\n",
       "      <td>494</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>484</td>\n",
       "      <td>479</td>\n",
       "      <td>574</td>\n",
       "      <td>509</td>\n",
       "      <td>509</td>\n",
       "      <td>473</td>\n",
       "      <td>483</td>\n",
       "      <td>545</td>\n",
       "      <td>490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0     486     497     494     477     582     478     535     477     496   \n",
       "1     496     524     490     485     438     488     503     476     474   \n",
       "2     486     465     481     467     529     484     464     476     508   \n",
       "3     479     485     502     489     487     478     402     477     500   \n",
       "4     482     485     551     475     443     475     456     475     494   \n",
       "\n",
       "   feat_9  ...  feat_491  feat_492  feat_493  feat_494  feat_495  feat_496  \\\n",
       "0     480  ...       485       473       576       521       493       481   \n",
       "1     491  ...       474       519       441       453       488       488   \n",
       "2     474  ...       482       454       712       425       518       479   \n",
       "3     473  ...       470       491       381       532       469       488   \n",
       "4     471  ...       484       479       574       509       509       473   \n",
       "\n",
       "   feat_497  feat_498  feat_499  labels  \n",
       "0       485       490       478       0  \n",
       "1       503       543       547       0  \n",
       "2       466       494       470       1  \n",
       "3       487       539       546       1  \n",
       "4       483       545       490       1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=data.drop(columns='labels',axis=0).values\n",
    "Y_train=data['labels'].values\n",
    "X_test=test_data.drop(columns='labels',axis=0).values\n",
    "Y_test=test_data['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following is a baseline model without any feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 1.0\n",
      "Test accuracy 0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dtree_classifier=tree.DecisionTreeClassifier()\n",
    "dtree_classifier=dtree_classifier.fit(X_train,Y_train)\n",
    "y_train_pred=dtree_classifier.predict(X_train)\n",
    "y_test_pred=dtree_classifier.predict(X_test)\n",
    "print('Training accuracy',accuracy_score(y_train_pred,Y_train))\n",
    "print('Test accuracy',accuracy_score(y_test_pred,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection=['feat_493', 'feat_28', 'feat_105', 'feat_318', 'feat_153', 'feat_48', 'feat_442', 'feat_475', 'feat_378', 'feat_338']\n",
    "\n",
    "\n",
    "X_train=data.drop(columns='labels',axis=0)[feature_selection].values\n",
    "Y_train=data['labels'].values\n",
    "X_test=test_data.drop(columns='labels',axis=0)[feature_selection].values\n",
    "Y_test=test_data['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 1.0\n",
      "Test accuracy 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dtree_classifier=tree.DecisionTreeClassifier()\n",
    "dtree_classifier=dtree_classifier.fit(X_train,Y_train)\n",
    "y_train_pred=dtree_classifier.predict(X_train)\n",
    "y_test_pred=dtree_classifier.predict(X_test)\n",
    "print('Training accuracy',accuracy_score(y_train_pred,Y_train))\n",
    "print('Test accuracy',accuracy_score(y_test_pred,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Support Vectors Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt  \n",
    "import seaborn as sns\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>423</td>\n",
       "      <td>504</td>\n",
       "      <td>493</td>\n",
       "      <td>521</td>\n",
       "      <td>466</td>\n",
       "      <td>494</td>\n",
       "      <td>479</td>\n",
       "      <td>482</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>478</td>\n",
       "      <td>479</td>\n",
       "      <td>567</td>\n",
       "      <td>547</td>\n",
       "      <td>498</td>\n",
       "      <td>484</td>\n",
       "      <td>474</td>\n",
       "      <td>567</td>\n",
       "      <td>538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483</td>\n",
       "      <td>499</td>\n",
       "      <td>520</td>\n",
       "      <td>467</td>\n",
       "      <td>495</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>477</td>\n",
       "      <td>488</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>481</td>\n",
       "      <td>484</td>\n",
       "      <td>451</td>\n",
       "      <td>445</td>\n",
       "      <td>443</td>\n",
       "      <td>481</td>\n",
       "      <td>485</td>\n",
       "      <td>492</td>\n",
       "      <td>477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>487</td>\n",
       "      <td>486</td>\n",
       "      <td>495</td>\n",
       "      <td>481</td>\n",
       "      <td>421</td>\n",
       "      <td>481</td>\n",
       "      <td>499</td>\n",
       "      <td>478</td>\n",
       "      <td>489</td>\n",
       "      <td>482</td>\n",
       "      <td>...</td>\n",
       "      <td>477</td>\n",
       "      <td>511</td>\n",
       "      <td>245</td>\n",
       "      <td>522</td>\n",
       "      <td>480</td>\n",
       "      <td>483</td>\n",
       "      <td>493</td>\n",
       "      <td>421</td>\n",
       "      <td>488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480</td>\n",
       "      <td>427</td>\n",
       "      <td>531</td>\n",
       "      <td>458</td>\n",
       "      <td>544</td>\n",
       "      <td>492</td>\n",
       "      <td>489</td>\n",
       "      <td>477</td>\n",
       "      <td>478</td>\n",
       "      <td>482</td>\n",
       "      <td>...</td>\n",
       "      <td>483</td>\n",
       "      <td>471</td>\n",
       "      <td>313</td>\n",
       "      <td>490</td>\n",
       "      <td>414</td>\n",
       "      <td>480</td>\n",
       "      <td>516</td>\n",
       "      <td>495</td>\n",
       "      <td>469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>491</td>\n",
       "      <td>472</td>\n",
       "      <td>430</td>\n",
       "      <td>463</td>\n",
       "      <td>431</td>\n",
       "      <td>480</td>\n",
       "      <td>459</td>\n",
       "      <td>477</td>\n",
       "      <td>481</td>\n",
       "      <td>479</td>\n",
       "      <td>...</td>\n",
       "      <td>479</td>\n",
       "      <td>493</td>\n",
       "      <td>435</td>\n",
       "      <td>444</td>\n",
       "      <td>455</td>\n",
       "      <td>482</td>\n",
       "      <td>468</td>\n",
       "      <td>497</td>\n",
       "      <td>435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0     485     423     504     493     521     466     494     479     482   \n",
       "1     483     499     520     467     495     484     485     477     488   \n",
       "2     487     486     495     481     421     481     499     478     489   \n",
       "3     480     427     531     458     544     492     489     477     478   \n",
       "4     491     472     430     463     431     480     459     477     481   \n",
       "\n",
       "   feat_9  ...  feat_491  feat_492  feat_493  feat_494  feat_495  feat_496  \\\n",
       "0     471  ...       478       479       567       547       498       484   \n",
       "1     491  ...       481       484       451       445       443       481   \n",
       "2     482  ...       477       511       245       522       480       483   \n",
       "3     482  ...       483       471       313       490       414       480   \n",
       "4     479  ...       479       493       435       444       455       482   \n",
       "\n",
       "   feat_497  feat_498  feat_499  labels  \n",
       "0       474       567       538       0  \n",
       "1       485       492       477       1  \n",
       "2       493       421       488       1  \n",
       "3       516       495       469       0  \n",
       "4       468       497       435       1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('a4-train.csv', index_col=0)\n",
    "test_data=pd.read_csv('a4-test.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>486</td>\n",
       "      <td>497</td>\n",
       "      <td>494</td>\n",
       "      <td>477</td>\n",
       "      <td>582</td>\n",
       "      <td>478</td>\n",
       "      <td>535</td>\n",
       "      <td>477</td>\n",
       "      <td>496</td>\n",
       "      <td>480</td>\n",
       "      <td>...</td>\n",
       "      <td>485</td>\n",
       "      <td>473</td>\n",
       "      <td>576</td>\n",
       "      <td>521</td>\n",
       "      <td>493</td>\n",
       "      <td>481</td>\n",
       "      <td>485</td>\n",
       "      <td>490</td>\n",
       "      <td>478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>496</td>\n",
       "      <td>524</td>\n",
       "      <td>490</td>\n",
       "      <td>485</td>\n",
       "      <td>438</td>\n",
       "      <td>488</td>\n",
       "      <td>503</td>\n",
       "      <td>476</td>\n",
       "      <td>474</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>474</td>\n",
       "      <td>519</td>\n",
       "      <td>441</td>\n",
       "      <td>453</td>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "      <td>503</td>\n",
       "      <td>543</td>\n",
       "      <td>547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>486</td>\n",
       "      <td>465</td>\n",
       "      <td>481</td>\n",
       "      <td>467</td>\n",
       "      <td>529</td>\n",
       "      <td>484</td>\n",
       "      <td>464</td>\n",
       "      <td>476</td>\n",
       "      <td>508</td>\n",
       "      <td>474</td>\n",
       "      <td>...</td>\n",
       "      <td>482</td>\n",
       "      <td>454</td>\n",
       "      <td>712</td>\n",
       "      <td>425</td>\n",
       "      <td>518</td>\n",
       "      <td>479</td>\n",
       "      <td>466</td>\n",
       "      <td>494</td>\n",
       "      <td>470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479</td>\n",
       "      <td>485</td>\n",
       "      <td>502</td>\n",
       "      <td>489</td>\n",
       "      <td>487</td>\n",
       "      <td>478</td>\n",
       "      <td>402</td>\n",
       "      <td>477</td>\n",
       "      <td>500</td>\n",
       "      <td>473</td>\n",
       "      <td>...</td>\n",
       "      <td>470</td>\n",
       "      <td>491</td>\n",
       "      <td>381</td>\n",
       "      <td>532</td>\n",
       "      <td>469</td>\n",
       "      <td>488</td>\n",
       "      <td>487</td>\n",
       "      <td>539</td>\n",
       "      <td>546</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>482</td>\n",
       "      <td>485</td>\n",
       "      <td>551</td>\n",
       "      <td>475</td>\n",
       "      <td>443</td>\n",
       "      <td>475</td>\n",
       "      <td>456</td>\n",
       "      <td>475</td>\n",
       "      <td>494</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>484</td>\n",
       "      <td>479</td>\n",
       "      <td>574</td>\n",
       "      <td>509</td>\n",
       "      <td>509</td>\n",
       "      <td>473</td>\n",
       "      <td>483</td>\n",
       "      <td>545</td>\n",
       "      <td>490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0     486     497     494     477     582     478     535     477     496   \n",
       "1     496     524     490     485     438     488     503     476     474   \n",
       "2     486     465     481     467     529     484     464     476     508   \n",
       "3     479     485     502     489     487     478     402     477     500   \n",
       "4     482     485     551     475     443     475     456     475     494   \n",
       "\n",
       "   feat_9  ...  feat_491  feat_492  feat_493  feat_494  feat_495  feat_496  \\\n",
       "0     480  ...       485       473       576       521       493       481   \n",
       "1     491  ...       474       519       441       453       488       488   \n",
       "2     474  ...       482       454       712       425       518       479   \n",
       "3     473  ...       470       491       381       532       469       488   \n",
       "4     471  ...       484       479       574       509       509       473   \n",
       "\n",
       "   feat_497  feat_498  feat_499  labels  \n",
       "0       485       490       478       0  \n",
       "1       503       543       547       0  \n",
       "2       466       494       470       1  \n",
       "3       487       539       546       1  \n",
       "4       483       545       490       1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=data.drop(columns='labels',axis=0).values\n",
    "Y_train=data['labels'].values\n",
    "X_test=test_data.drop(columns='labels',axis=0).values\n",
    "Y_test=test_data['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9715\n",
      "0.5816666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "classifier=SVC()\n",
    "classifier.fit(X_train,Y_train)\n",
    "y_train_pred=classifier.predict(X_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "print(accuracy_score(y_train_pred,Y_train))\n",
    "print(accuracy_score(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection=['feat_493', 'feat_28', 'feat_105', 'feat_318', 'feat_153', 'feat_48', 'feat_442', 'feat_475', 'feat_378', 'feat_338']\n",
    "\n",
    "\n",
    "X_train=data.drop(columns='labels',axis=0)[feature_selection].values\n",
    "Y_train=data['labels'].values\n",
    "X_test=test_data.drop(columns='labels',axis=0)[feature_selection].values\n",
    "Y_test=test_data['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.87\n",
      "Test accuracy 0.865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "classifier=SVC()\n",
    "classifier.fit(X_train,Y_train)\n",
    "y_train_pred=classifier.predict(X_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "print('Training accuracy',accuracy_score(y_train_pred,Y_train))\n",
    "print('Test accuracy',accuracy_score(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) K- Nearest Neighbours(KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt  \n",
    "import seaborn as sns\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>423</td>\n",
       "      <td>504</td>\n",
       "      <td>493</td>\n",
       "      <td>521</td>\n",
       "      <td>466</td>\n",
       "      <td>494</td>\n",
       "      <td>479</td>\n",
       "      <td>482</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>478</td>\n",
       "      <td>479</td>\n",
       "      <td>567</td>\n",
       "      <td>547</td>\n",
       "      <td>498</td>\n",
       "      <td>484</td>\n",
       "      <td>474</td>\n",
       "      <td>567</td>\n",
       "      <td>538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483</td>\n",
       "      <td>499</td>\n",
       "      <td>520</td>\n",
       "      <td>467</td>\n",
       "      <td>495</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>477</td>\n",
       "      <td>488</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>481</td>\n",
       "      <td>484</td>\n",
       "      <td>451</td>\n",
       "      <td>445</td>\n",
       "      <td>443</td>\n",
       "      <td>481</td>\n",
       "      <td>485</td>\n",
       "      <td>492</td>\n",
       "      <td>477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>487</td>\n",
       "      <td>486</td>\n",
       "      <td>495</td>\n",
       "      <td>481</td>\n",
       "      <td>421</td>\n",
       "      <td>481</td>\n",
       "      <td>499</td>\n",
       "      <td>478</td>\n",
       "      <td>489</td>\n",
       "      <td>482</td>\n",
       "      <td>...</td>\n",
       "      <td>477</td>\n",
       "      <td>511</td>\n",
       "      <td>245</td>\n",
       "      <td>522</td>\n",
       "      <td>480</td>\n",
       "      <td>483</td>\n",
       "      <td>493</td>\n",
       "      <td>421</td>\n",
       "      <td>488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480</td>\n",
       "      <td>427</td>\n",
       "      <td>531</td>\n",
       "      <td>458</td>\n",
       "      <td>544</td>\n",
       "      <td>492</td>\n",
       "      <td>489</td>\n",
       "      <td>477</td>\n",
       "      <td>478</td>\n",
       "      <td>482</td>\n",
       "      <td>...</td>\n",
       "      <td>483</td>\n",
       "      <td>471</td>\n",
       "      <td>313</td>\n",
       "      <td>490</td>\n",
       "      <td>414</td>\n",
       "      <td>480</td>\n",
       "      <td>516</td>\n",
       "      <td>495</td>\n",
       "      <td>469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>491</td>\n",
       "      <td>472</td>\n",
       "      <td>430</td>\n",
       "      <td>463</td>\n",
       "      <td>431</td>\n",
       "      <td>480</td>\n",
       "      <td>459</td>\n",
       "      <td>477</td>\n",
       "      <td>481</td>\n",
       "      <td>479</td>\n",
       "      <td>...</td>\n",
       "      <td>479</td>\n",
       "      <td>493</td>\n",
       "      <td>435</td>\n",
       "      <td>444</td>\n",
       "      <td>455</td>\n",
       "      <td>482</td>\n",
       "      <td>468</td>\n",
       "      <td>497</td>\n",
       "      <td>435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0     485     423     504     493     521     466     494     479     482   \n",
       "1     483     499     520     467     495     484     485     477     488   \n",
       "2     487     486     495     481     421     481     499     478     489   \n",
       "3     480     427     531     458     544     492     489     477     478   \n",
       "4     491     472     430     463     431     480     459     477     481   \n",
       "\n",
       "   feat_9  ...  feat_491  feat_492  feat_493  feat_494  feat_495  feat_496  \\\n",
       "0     471  ...       478       479       567       547       498       484   \n",
       "1     491  ...       481       484       451       445       443       481   \n",
       "2     482  ...       477       511       245       522       480       483   \n",
       "3     482  ...       483       471       313       490       414       480   \n",
       "4     479  ...       479       493       435       444       455       482   \n",
       "\n",
       "   feat_497  feat_498  feat_499  labels  \n",
       "0       474       567       538       0  \n",
       "1       485       492       477       1  \n",
       "2       493       421       488       1  \n",
       "3       516       495       469       0  \n",
       "4       468       497       435       1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('a4-train.csv', index_col=0)\n",
    "test_data=pd.read_csv('a4-test.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>486</td>\n",
       "      <td>497</td>\n",
       "      <td>494</td>\n",
       "      <td>477</td>\n",
       "      <td>582</td>\n",
       "      <td>478</td>\n",
       "      <td>535</td>\n",
       "      <td>477</td>\n",
       "      <td>496</td>\n",
       "      <td>480</td>\n",
       "      <td>...</td>\n",
       "      <td>485</td>\n",
       "      <td>473</td>\n",
       "      <td>576</td>\n",
       "      <td>521</td>\n",
       "      <td>493</td>\n",
       "      <td>481</td>\n",
       "      <td>485</td>\n",
       "      <td>490</td>\n",
       "      <td>478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>496</td>\n",
       "      <td>524</td>\n",
       "      <td>490</td>\n",
       "      <td>485</td>\n",
       "      <td>438</td>\n",
       "      <td>488</td>\n",
       "      <td>503</td>\n",
       "      <td>476</td>\n",
       "      <td>474</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>474</td>\n",
       "      <td>519</td>\n",
       "      <td>441</td>\n",
       "      <td>453</td>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "      <td>503</td>\n",
       "      <td>543</td>\n",
       "      <td>547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>486</td>\n",
       "      <td>465</td>\n",
       "      <td>481</td>\n",
       "      <td>467</td>\n",
       "      <td>529</td>\n",
       "      <td>484</td>\n",
       "      <td>464</td>\n",
       "      <td>476</td>\n",
       "      <td>508</td>\n",
       "      <td>474</td>\n",
       "      <td>...</td>\n",
       "      <td>482</td>\n",
       "      <td>454</td>\n",
       "      <td>712</td>\n",
       "      <td>425</td>\n",
       "      <td>518</td>\n",
       "      <td>479</td>\n",
       "      <td>466</td>\n",
       "      <td>494</td>\n",
       "      <td>470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479</td>\n",
       "      <td>485</td>\n",
       "      <td>502</td>\n",
       "      <td>489</td>\n",
       "      <td>487</td>\n",
       "      <td>478</td>\n",
       "      <td>402</td>\n",
       "      <td>477</td>\n",
       "      <td>500</td>\n",
       "      <td>473</td>\n",
       "      <td>...</td>\n",
       "      <td>470</td>\n",
       "      <td>491</td>\n",
       "      <td>381</td>\n",
       "      <td>532</td>\n",
       "      <td>469</td>\n",
       "      <td>488</td>\n",
       "      <td>487</td>\n",
       "      <td>539</td>\n",
       "      <td>546</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>482</td>\n",
       "      <td>485</td>\n",
       "      <td>551</td>\n",
       "      <td>475</td>\n",
       "      <td>443</td>\n",
       "      <td>475</td>\n",
       "      <td>456</td>\n",
       "      <td>475</td>\n",
       "      <td>494</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>484</td>\n",
       "      <td>479</td>\n",
       "      <td>574</td>\n",
       "      <td>509</td>\n",
       "      <td>509</td>\n",
       "      <td>473</td>\n",
       "      <td>483</td>\n",
       "      <td>545</td>\n",
       "      <td>490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0     486     497     494     477     582     478     535     477     496   \n",
       "1     496     524     490     485     438     488     503     476     474   \n",
       "2     486     465     481     467     529     484     464     476     508   \n",
       "3     479     485     502     489     487     478     402     477     500   \n",
       "4     482     485     551     475     443     475     456     475     494   \n",
       "\n",
       "   feat_9  ...  feat_491  feat_492  feat_493  feat_494  feat_495  feat_496  \\\n",
       "0     480  ...       485       473       576       521       493       481   \n",
       "1     491  ...       474       519       441       453       488       488   \n",
       "2     474  ...       482       454       712       425       518       479   \n",
       "3     473  ...       470       491       381       532       469       488   \n",
       "4     471  ...       484       479       574       509       509       473   \n",
       "\n",
       "   feat_497  feat_498  feat_499  labels  \n",
       "0       485       490       478       0  \n",
       "1       503       543       547       0  \n",
       "2       466       494       470       1  \n",
       "3       487       539       546       1  \n",
       "4       483       545       490       1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=data.drop(columns='labels',axis=0).values\n",
    "Y_train=data['labels'].values\n",
    "X_test=test_data.drop(columns='labels',axis=0).values\n",
    "Y_test=test_data['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.729\n",
      "Test accuracy 0.5033333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "X_train=np.nan_to_num(X_train)\n",
    "X_test=np.nan_to_num(X_test)\n",
    "Y_train=np.nan_to_num(Y_train)\n",
    "Y_test=np.nan_to_num(Y_test)\n",
    "\n",
    "knn_classifier=KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train,Y_train)\n",
    "y_pred_train=knn_classifier.predict(X_train)\n",
    "y_pred_test=knn_classifier.predict(X_test)\n",
    "\n",
    "print(\"Training accuracy\",accuracy_score(y_pred_train,Y_train))\n",
    "print(\"Test accuracy\",accuracy_score(y_pred_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection=['feat_493', 'feat_28', 'feat_105', 'feat_318', 'feat_153', 'feat_48', 'feat_442', 'feat_475', 'feat_378', 'feat_338']\n",
    "#feature_selection=['feat_255', 'feat_17', 'feat_164', 'feat_275', 'feat_364', 'feat_3', 'feat_4', 'feat_1', 'feat_2', 'feat_0']\n",
    "\n",
    "X_train=data.drop(columns='labels',axis=0)[feature_selection].values\n",
    "Y_train=data['labels'].values\n",
    "X_test=test_data.drop(columns='labels',axis=0)[feature_selection].values\n",
    "Y_test=test_data['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.928\n",
      "Test accuracy 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "X_train=np.nan_to_num(X_train)\n",
    "X_test=np.nan_to_num(X_test)\n",
    "Y_train=np.nan_to_num(Y_train)\n",
    "Y_test=np.nan_to_num(Y_test)\n",
    "\n",
    "knn_classifier=KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train,Y_train)\n",
    "y_pred_train=knn_classifier.predict(X_train)\n",
    "y_pred_test=knn_classifier.predict(X_test)\n",
    "\n",
    "print(\"Training accuracy\",accuracy_score(y_pred_train,Y_train))\n",
    "print(\"Test accuracy\",accuracy_score(y_pred_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working of Model4-\n",
    "\n",
    "Above I have implemented the K-Nearest Neighbours(KNN) algorithm using sklearn, for classifying samples into two classes (1 or 0). The “K” is KNN algorithm is the nearest neighbors we wish to take vote from. For example- Here, the default value of K is 5. Now, to classify a data point into either of classes, the algorithm builds a circle with the given data point as center just as big as to enclose only five datapoints. Finally, to make a decision, the algorithm calculates the class majority among these five data points, and it then assigns the given data point based on the majority.\n",
    "\n",
    "During training, the model builds a decision boundary using the approach discussed above, while during evaluation, the model classifies a data point based the decision boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Here's a brief summary of accuracy of models which I implemented-\n",
    "\n",
    "1) Neural Networks-\n",
    "a) Without feature engineering- Training Accuracy- 100%, Test Accuracy- 56.5%\n",
    "b) With feature engineering- Training Accuracy- 89.45%, Test Accuracy- 85.83%\n",
    "\n",
    "2) Decision Trees-\n",
    "a) Without feature engineering- Training Accuracy- 100, Test Accuracy-73%\n",
    "b) With feature engineering- Training Accuracy- 100, Test Accuracy- 83.33%\n",
    "\n",
    "3) SVM-\n",
    "a) Without feature engineering- Training Accuracy- 97.15%, Test Accuracy- 58.16%\n",
    "b) With feature engineering- Training Accuracy- 87%, Test Accuracy- 86.5%\n",
    "\n",
    "4) KNN-\n",
    "a) Without feature engineering- Training Accuracy- 72.9%, Test Accuracy- 50.33%\n",
    "b) With feature engineering- Training Accuracy- 92.8%, Test Accuracy- 91%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "assignment4-F19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
